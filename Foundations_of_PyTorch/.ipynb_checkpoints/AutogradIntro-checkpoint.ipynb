{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1979de71",
   "metadata": {},
   "source": [
    "# Demo: Introducing Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c3d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first understand how tensors can be set up to track history using the Autograd package.\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d6827de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's instantiate a couple of tensors;\n",
    "\n",
    "# 2 x 3 tensor\n",
    "tensor1 = torch.Tensor([[1, 2, 3 ],\n",
    "                        [4, 5, 6]\n",
    "                       ])\n",
    "\n",
    "display(tensor1.shape)\n",
    "\n",
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fba12cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.,  8.,  9.],\n",
       "        [10., 11., 12.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another 2 x 3 tensor\n",
    "tensor2 = torch.Tensor([[7, 8, 9],\n",
    "                       [10, 11, 12]])\n",
    "\n",
    "tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6498044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Every tensor created in PyTorch will have the \"requires_grad\" property. When the value of this is true,\n",
    "# this means that PyTorch will track computations for this tensor in the forward phase when we use the \n",
    "# computation graph to make predictions, and it will calculate gradients for this tensor in the backward phase.\n",
    "# The gradients will be calculated with respect to a scalar, such as the loss. \n",
    "\n",
    "# When you instantiate a tensor, the default value for requires_grad is set to false.\n",
    "# Let's check the requires_grad property for tensor2 and you'll find that this is false as well.\n",
    "\n",
    "display(tensor1.requires_grad)\n",
    "display(tensor2.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdf2b47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to enable tracking history for a particular tensor so that gradients are calculated with respect to that tensor,\n",
    "# you need to enable the requires_grad flag, which you can do by calling requires_grad_() function.\n",
    "\n",
    "tensor1.requires_grad_()\n",
    "\n",
    "# now the property is true\n",
    "display(tensor1.requires_grad)\n",
    "\n",
    "\n",
    "# Calling requires_grad_() on a tesnor will update the requires_grad property for this tensor in place.\n",
    "# You can confirm this by checking the requires_grad property.\n",
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caf4916b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you check the requires_grad property for tensor2, you'll find that it's still false.\n",
    "\n",
    "tensor2.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9064a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# The gradients calculated using automatic differentiation with respect to any tensor is present in the grad matrix\n",
    "# associated with the tensor.\n",
    "\n",
    "# no gradients are available yet - this is part of a computation graph but no forward or backward passes have been made\n",
    "print(tensor1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05de3866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can see that there are no gradients available yet.\n",
    "# We have set up a tensor, but we haven't used it within a computation graph.\n",
    "# We haven't performed a forward or a backward pass.\n",
    "# This tensor hasn't been used in a computation.\n",
    "# There is nothing to calculate gradients with respect to.\n",
    "\n",
    "display(tensor1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cc1172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# The computation graph within PyTorch is made up of tensors and functions.\n",
    "# These together make up our directed acyclic computation graph.\n",
    "\n",
    "# Directed acyclic computation graphs are often used in deep learning frameworks such as TensorFlow and PyTorch \n",
    "# to represent the flow of data through the layers of a neural network. \n",
    "# This representation helps perform automatic differentiation, allowing the efficient computation of gradients \n",
    "# for optimization.\n",
    "\n",
    "# You can think of tensors as the nodes in this graph and functions are the transformations performed along edges.\n",
    "# Every tensor has a grad function used to create that function (i.e. transformations or affine transformation functions).\n",
    "# Now this tensor is something that we created, it's grad function is none.\n",
    "\n",
    "print(tensor1.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "190fe1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.,  8.,  9.],\n",
       "        [10., 11., 12.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7., 16., 27.],\n",
       "        [40., 55., 72.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(tensor1)\n",
    "display(tensor2)\n",
    "\n",
    "# Let's now use these tensors to perform a simple computation\n",
    "# that will set up our computation graph.\n",
    "# Output_tensor is equal to tensor1 multiplied by tensor2.\n",
    "\n",
    "output_tensor = tensor1 * tensor2\n",
    "\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86a6ad32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# When you create a tensor using an operation,\n",
    "# the requires_grad property for the resulting tensor is based on the\n",
    "# input tensors that we use to create this output tensor.\n",
    "\n",
    "# Requires_grad is true because tensor1 had requires_grad set to true and\n",
    "# tensor1 was used to create this output tensor.\n",
    "display(output_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce087479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CavaJ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten\\src\\ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "# The output tensor also has the grad property used to store gradients.\n",
    "# There are no gradients, we haven't made any backward pass.\n",
    "\n",
    "print(output_tensor.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d712312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MulBackward0 object at 0x00000244A8584748>\n"
     ]
    }
   ],
   "source": [
    "# But you'll find that this output tensor will have a grad function, i.e. grad_fn.\n",
    "# Because we used a specific multiplication operation to create this output tensor,\n",
    "# you can see that MulBackward0 is the grad function associated with this tensor.\n",
    "\n",
    "# In PyTorch, any resulting tensor has reference to the function (operation) that created it.\n",
    "print(output_tensor.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eca44a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# User created tensors, such as tensor1 and tensor2,\n",
    "# will have no corresponding function.\n",
    "# And so one has no grad function, neither does tensor2.\n",
    "\n",
    "display(tensor1.grad_fn)\n",
    "display(tensor2.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37e98b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36.1667, grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MeanBackward0 object at 0x00000244AA32F470>\n"
     ]
    }
   ],
   "source": [
    "# We'll now create another output tensor by performing a slightly different operation, (tensor1 multiplied by tensor2).mean().\n",
    "# The grad function of the output tensor will reference the last function used to create that tensor, MeanBackward.\n",
    "\n",
    "# mean() function will find mean() of all elements in the resulting tensor.\n",
    "# Even though there were two functions involved, the multiplication and then the mean calculation,\n",
    "# the grad function references the last function, that is the mean.\n",
    "\n",
    "output_tensor = (tensor1 * tensor2).mean()\n",
    "display(output_tensor)\n",
    "print(output_tensor.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f98596d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# So tensor1 has been a part of multiple computation graphs where we calculated the output tensor,\n",
    "# but no gradient is associated yet with tensor1.\n",
    "# This is because we haven't yet performed a backward pass used to calculate gradients.\n",
    "print(tensor1.grad)\n",
    "\n",
    "# Gradient calculation, that is our vector of partial derivatives,\n",
    "# will be calculated only when we call the backward() function on an output.\n",
    "# So output_tensor.backward() will begin the backward pass through our\n",
    "# computation graph and now we'll have gradients for tensor1.\n",
    "output_tensor.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5ffbb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1667, 1.3333, 1.5000],\n",
      "        [1.6667, 1.8333, 2.0000]])\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CavaJ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten\\src\\ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "# Now we have the gradients for tensor1\n",
    "print(tensor1.grad)\n",
    "\n",
    "# nothing for tensor2, because its requires_grad property is set to false\n",
    "print(tensor2.grad)\n",
    "\n",
    "# output_tensor is also non-leaf tensor, there is no gradient calculated for it as well\n",
    "print(output_tensor.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82e7cd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1667, 1.3333, 1.5000],\n",
      "        [1.6667, 1.8333, 2.0000]])\n",
      "torch.Size([2, 3]) torch.Size([2, 3])\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# These gradients here are the partial derivatives for the parameters in tensor1 calculated \n",
    "# with reference to the output tensor. Since gradients are partial derivatives with respect to every value within tensor1,\n",
    "# the shape of the gradient will exactly match the shape of the tensor itself.\n",
    "print(tensor1.grad)\n",
    "print(tensor1.grad.shape, tensor1.shape)\n",
    "\n",
    "\n",
    "# Remember that tensor2 was also part of the computation,\n",
    "# but because requires_grad was set to false,\n",
    "# no gradients were calculated with respect to tensor2.\n",
    "print(tensor2.grad)\n",
    "\n",
    "\n",
    "# There are no gradients associated with the output tensor because\n",
    "# this is the value with respect to which we calculated partial\n",
    "# derivatives to get our gradients.\n",
    "print(output_tensor.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "baa26392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Once again, we'll calculate a new tensor using tensor1 and you'll see\n",
    "# how the requires_grad property is propagated from input\n",
    "# tensors to the result tensors.\n",
    "\n",
    "# The requires_grad property is propagated from input tensors to the resulting tensors.\n",
    "# You can see that new tensor has requires_grad set to true.\n",
    "new_tensor = tensor1 * 3\n",
    "print(new_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06075e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MulBackward0 object at 0x00000244AA32F160>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  6.,  9.],\n",
       "        [12., 15., 18.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And the gradient function, i.e. grad_fn associated with this new tensor is MulBackward,\n",
    "# the multiplication operation that created this tensor.\n",
    "print(new_tensor.grad_fn)\n",
    "\n",
    "new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31b31427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_tensor =  tensor([[ 3.,  6.,  9.],\n",
      "        [12., 15., 18.]])\n",
      "requires_grad for tensor1 =  True\n",
      "requires_grad for tensor2 =  False\n",
      "requires_grad for new_tensor =  False\n"
     ]
    }
   ],
   "source": [
    "# If you have tensors with requires_grad set to true, which means tracking history is enabled,\n",
    "# if you want to stop the Autograd package from tracking history on these tensors,\n",
    "# it's possible to do that using torch.no_grad.\n",
    "\n",
    "# Here we have a with block with torch.no_grad() and any computation performed within \n",
    "# this with block will have tracking history turned off.\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Here is a new tensor created from tensor1 which has requires_grad set to true.\n",
    "    new_tensor = tensor1 * 3\n",
    "    print('new_tensor = ', new_tensor)\n",
    "    # The original tensor, tensor1, has requires_grad set to true, there are no surprises here.\n",
    "    # For tensor2, it is false, because requires_grad is not enabled when the tensor is created.\n",
    "    print(\"requires_grad for tensor1 = \", tensor1.requires_grad) # it will be true, since it is explicitly set before with block\n",
    "    print(\"requires_grad for tensor2 = \", tensor2.requires_grad) # it is false, because it was false before with block\n",
    "    # but the new tensor that we created has requires_grad set to false.\n",
    "    # So requires_grad was not propagated to this new tensor because it was created within a torch.no_grad block.\n",
    "    print(\"requires_grad for new_tensor = \", new_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e09eb35",
   "metadata": {},
   "source": [
    "# Demo: Working with Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "931dcd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at a few Python functions and how we can use decorators\n",
    "# to determine whether gradients should be enabled or not.\n",
    "# Here is a function called calculate, which takes in a tensor t,\n",
    "# it multiplies t by 2 and returns a result.\n",
    "\n",
    "def calculate(t):\n",
    "    return t * 2\n",
    "\n",
    "\n",
    "# Now let's take a look at another function here,\n",
    "# calculate_with_no_grad, which has the decorator @ torch.no_grad applied.\n",
    "# You can see that calculate_with_no_grad performs the same\n",
    "# multiplication action as calculate, but gradients will not be enabled,\n",
    "# history tracking will not be turned on,\n",
    "# even if the tensor's requires_grad property set to true.\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_with_no_grad(t):\n",
    "    return t * 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "daabc787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.,  6.],\n",
       "        [ 8., 10., 12.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original tensor1 with requires_grad property set to true\n",
    "print(tensor1)\n",
    "\n",
    "\n",
    "# Let's now invoke first, the calculate function and pass in tensor1,\n",
    "# which has requires_grad set to true.\n",
    "# The result_tensor will also have gradients enabled and it's\n",
    "# grad function will be MulBackward because it was created\n",
    "# using a multiplication operation.\n",
    "result_tensor = calculate(tensor1)\n",
    "\n",
    "result_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a57a57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you check requires_grad for the result_tensor, you'll see that it's true.\n",
    "result_tensor.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb923a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.,  6.],\n",
       "        [ 8., 10., 12.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's calculate the result_tensor once again.\n",
    "# This time we'll call it result_tensor_no_grad,\n",
    "# but we'll invoke the calculate_with_no_grad function to do this.\n",
    "# This is the function with a decorator @ torch.no_grad,\n",
    "# which means gradient tracking will be turned off.\n",
    "# If you take a look at the result_tensor_no_grad,\n",
    "# you'll find that because history tracking was turned off,\n",
    "# it does not keep track of the function that created this\n",
    "# tensor and requires_grad is set to false,\n",
    "# even though the input tensor, tensor1, had requires_grad true.\n",
    "\n",
    "result_tensor_no_grad = calculate_with_no_grad(tensor1)\n",
    "\n",
    "display(result_tensor_no_grad)\n",
    "\n",
    "result_tensor_no_grad.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1505961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "new_tensor_no_grad =  tensor([[ 3.,  6.,  9.],\n",
      "        [12., 15., 18.]])\n",
      "new_tensor_grad =  tensor([[ 3.,  6.,  9.],\n",
      "        [12., 15., 18.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# tensor1 originally has requires_grad property set to true\n",
    "print(tensor1.requires_grad)\n",
    "\n",
    "# Just like we can turn off history tracking using torch.no_grad,\n",
    "# we can enable explicit tracking of history within a torch.no_grad() block.\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # So here is a tensor computation, tensor1 multiplied by 3,\n",
    "    # stored in new_tensor_no_grad, which is within our outer torch.no_grad() block.\n",
    "    new_tensor_no_grad = tensor1 * 3\n",
    "    print('new_tensor_no_grad = ', new_tensor_no_grad)\n",
    "    \n",
    "    # But we want to turn off gradient tracking in general,\n",
    "    # within this outer block we want to enable gradients for one computation.\n",
    "    # You can do that using with torch.enable_grad() within an outer no_grad block.\n",
    "    with torch.enable_grad():\n",
    "        # Here we compute tensor1 multiplied by 3 and store the result in new_tensor_grad.\n",
    "        new_tensor_grad = tensor1 * 3\n",
    "        print('new_tensor_grad = ', new_tensor_grad)\n",
    "\n",
    "        \n",
    "# This is how you can nest an inner block where you want\n",
    "# history tracking to be enabled and gradients to be calculated\n",
    "# within an outer torch.no_grad block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "252550a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  6.,  9.],\n",
       "        [12., 15., 18.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  6.,  9.],\n",
       "        [12., 15., 18.]], grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If you take a look at the result,\n",
    "# you can see that the tensor that was created within the\n",
    "# torch.no_grad outer block does not have grad function and\n",
    "# its requires_grad will be false.\n",
    "display(new_tensor_no_grad)\n",
    "display(new_tensor_no_grad.requires_grad)\n",
    "\n",
    "# The tensor that was created within the inner block with torch.enable_grad has\n",
    "# a grad function and its requires_grad will be set to true.\n",
    "display(new_tensor_grad)\n",
    "display(new_tensor_grad.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e628ac00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also specify the value for requires_grad when you instantiate a tensor.\n",
    "# Here is a torch.tensor created with requires.grad set to true, that is tensor_one.\n",
    "tensor_one = torch.tensor([[1.0, 2.0],\n",
    "                          [3.0, 4.0]], requires_grad=True)\n",
    "\n",
    "tensor_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d78d09fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 6.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll create tensor_two using torch.Tensor, by default requires_grad will be set to false.\n",
    "tensor_two = torch.Tensor([[5, 6],\n",
    "                          [7, 8]])\n",
    "\n",
    "tensor_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1d2a979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5., 6.],\n",
       "        [7., 8.]], requires_grad=True)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor_one already has history tracking and gradient calculation enabled.\n",
    "print(tensor_one.requires_grad)\n",
    "\n",
    "# Let's do the same for tensor_two by calling requires_grad_(),\n",
    "# which will update the requires_grad flag in place.\n",
    "\n",
    "tensor_two.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b60d477b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll now perform a simple computation which sets up the\n",
    "# forward pass of our computation graph, tensor_one + tensor_two, \n",
    "# the whole thing will calculate the mean and store in final_tensor.\n",
    "\n",
    "final_tensor = (tensor_one + tensor_two).mean()\n",
    "final_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3dc79b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MeanBackward0 at 0x244aa33a048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can see that the final_tensor has the grad function MeanBackward associated.\n",
    "# The requires_grad property for the final tensor is true because\n",
    "# this is propagated from the input tensors.\n",
    "display(final_tensor.grad_fn)\n",
    "display(final_tensor.requires_grad)\n",
    "\n",
    "# If any of the input tensors have requires_grad set to true,\n",
    "# final_tensor will also have its requires_grad property set to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d0b9ccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# We'll only perform the forward pass on our computation graphs.\n",
    "# There are no gradients (i.e. grad matrix) associated with tensor_one, nor with tensor_two.\n",
    "\n",
    "print(tensor_one.grad)\n",
    "print(tensor_two.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a229be57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n",
      "tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "# We have history tracking for all of the tensors in this computation graph.\n",
    "# When we call final_tensor.backward(),\n",
    "# that's when gradients are calculated and you'll now see that\n",
    "# tensor_one has corresponding gradients, as does tensor_two.\n",
    "\n",
    "final_tensor.backward()\n",
    "\n",
    "# These are gradients of tensor_one and tensor_two\n",
    "# calculated with respect to the final tensor.\n",
    "print(tensor_one.grad)\n",
    "print(tensor_two.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6ed5ab2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors involved in a computation are part of a larger computation graph.\n",
    "# If you want a tensor which is detached from the current computation graph,\n",
    "# you can call the detach() function on a tensor that will return a new\n",
    "# tensor detached from the current computation graph,\n",
    "# and this new tensor will always have requires_grad set to false.\n",
    "\n",
    "# detach() returns a new tensor detached from the current computation graph - will always have requires_grad=False\n",
    "detached_tensor = tensor_one.detach()\n",
    "detached_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a2f97e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remember that the original tensor, tensor_one, has requires_grad set to true.\n",
    "display(tensor_one)\n",
    "\n",
    "# The detached_tensor has requires_grad set to false.\n",
    "# It's part of no computation graph.\n",
    "display(detached_tensor)\n",
    "display(detached_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4e573551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5., grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We'll now use the original tensor, tensor_one,\n",
    "# and the detached_tensor in the computation.\n",
    "# And we'll call the mean() function and the result is stored in mean_tensor.\n",
    "# the operation that creates mean_tensor actually constitutes our forward pass.\n",
    "mean_tensor = (tensor_one + detached_tensor).mean()\n",
    "\n",
    "# mean_tensor will have MeanBackward as grad function, propagated from the last operation that create the tensor\n",
    "display(mean_tensor)\n",
    "\n",
    "# it also has requires_grad set to true, again propagated from one the tensors which has requires_grad true.\n",
    "display(mean_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4ec6c359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000],\n",
       "        [0.5000, 0.5000]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With the forward pass complete,\n",
    "# we'll call mean_tensor.backward() to calculate gradients.\n",
    "mean_tensor.backward()\n",
    "\n",
    "# And if you take a look at the resulting tensors,\n",
    "# you can see that gradients have been calculated for tensor_one,\n",
    "# but no gradients have been calculated for the detached_tensor\n",
    "# because it has requires_grad set to false.\n",
    "display(tensor_one.grad)\n",
    "display(detached_tensor.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
