{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d4bb6c0",
   "metadata": {},
   "source": [
    "# Demo: Creating Tensors on CUDA-enabled devices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd17510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7abecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether the cuda is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8e20a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to initialize the CUDA state for PyTorch, you can call torch.cuda.init(). \n",
    "# This is required when interacting with PyTorch's C API. \n",
    "# When you are working with Python, the CUDA state is initilized on demand, so it's not really needed here in this case.\n",
    "\n",
    "torch.cuda.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e92c9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At any point in time, when you're working with PyTorch, torch.cuda keeps track of the currently selected GPU and \n",
    "# all CUDA tensors that you allocate will by default, be created on that device.\n",
    "\n",
    "# the gpus with indexed positions, it will return index of the current device\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b07bdecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of CUDA emabled devices available for PyTorch to use by running torch.cuda.device_count().\n",
    "\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "866fb8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to use PyTorch to monitor how much memory your tensors occupy, you can call torch.cuda.memory_allocated().\n",
    "\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a742f45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Behind the scenes PyTorch uses a caching memeroy allocator to speed up memory allocations to your tensors - \n",
    "# this allows fast memory deallocation without device synchronizations between your different CUDA devices.\n",
    "\n",
    "# FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
    "torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "747fc132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"cuda\" refers to the default CUDA device used by PyTorch (on which tensors will be created) - \n",
    "# this is something that can be changed using the device context manager.\n",
    "\n",
    "cuda = torch.device(\"cuda\")\n",
    "\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18305514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If you want to access to a specific CUDA device using the device's context manager, you will reference it using an index.\n",
    "\n",
    "# but we should have only one cuda device which is a dedicated GPU of your laptop.\n",
    "# So, cuda1 and cuda2 are not really valid.\n",
    "# The default cuda device is at index 0.\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "cuda2 = torch.device(\"cuda:2\")\n",
    "\n",
    "display(cuda0)\n",
    "display(cuda1)\n",
    "display(cuda2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62281840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 20.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When you create a torch tensor and you haven't specified a CUDA device, this tensor, by default, is created on the CPU.\n",
    "\n",
    "x = torch.tensor([10., 20.])\n",
    "\n",
    "# x is a tensor created on CPU, because it has not CUDA device associated with it.\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cf819c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 20.], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to create a tensor on GPU, you need to explicitly specify the device parameter, device equal to cuda\n",
    "# (meaning that it is going to use defauly cuda device)\n",
    "x_default = torch.tensor([10., 20.], device=cuda) # device=cuda means device=\"cuda:0\"\n",
    "\n",
    "x_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40fd6274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 20.], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = torch.tensor([10.0, 20.0], device=cuda0)\n",
    "\n",
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c913e4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bad45714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2097152"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0981bdf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4168\\3830432749.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# will result in an error, since there is no 2nd GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcuda1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# Let's create another tensor, x1, explicitly on the device cuda1.\n",
    "\n",
    "# will result in an error, since there is no 2nd GPU\n",
    "x1 = torch.tensor([10.0, 20.0], device=cuda1)\n",
    "\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e38cd063",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4168\\2199412544.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# The same happens for the 3rd cuda device we obtained a reference to.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcuda2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# The same happens for the 3rd cuda device we obtained a reference to.\n",
    "\n",
    "x2 = torch.tensor([10.0, 20.0], device=cuda2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a2e1c",
   "metadata": {},
   "source": [
    "# Demo: Working with the Device Context Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee4f495b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 20.], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to create a copy of an object in CUDA memory, you can call the cuda() function\n",
    "# If the tensor is already in CUDA memory and on the correct device, no copy is performed.\n",
    "\n",
    "# Let's copy our CPU tensor, x, to CUDA memory (to the default CUDA device).\n",
    "y = x.cuda()\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cb18022",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4168\\652850364.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# now create a copy of x1 (which was actually created on cuda:1) on default device cuda:0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# since we do not have second GPU, there is no tensor created on, so this operation will fail\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x1' is not defined"
     ]
    }
   ],
   "source": [
    "# now create a copy of x1 (which was actually created on cuda:1) on default device cuda:0\n",
    "y0 = x1.cuda() # since we do not have second GPU, there is no tensor created on, so this operation will fail\n",
    "\n",
    "y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a48c7b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside with context:  0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4168\\4216204056.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Only within this \"with\" block, the device context is 1, outside it is 0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# there is no device 1, so error will occur\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Inside with context: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_idx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[0m_lazy_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mset_device\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_setDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# You can use a with context in order to change the default device for PyTorch, using the device context manager.\n",
    "\n",
    "print(\"Outside with context: \", torch.cuda.current_device())\n",
    "\n",
    "# Only within this \"with\" block, the device context is 1, outside it is 0.\n",
    "with torch.cuda.device(1): # there is no device 1, so error will occur\n",
    "    print(\"Inside with context: \", torch.cuda.current_device())\n",
    "    \n",
    "print(\"Outside with context: \", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0dff623",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4168\\3876308156.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m# when we explicitly specify a device within a with context the tensor is created on the specified device,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# since we have not specified a cuda device here, tensor a will be created on CPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# here, we explicitly create it on cuda:0, so, it will not be in cuda:1 as we specifed by context manager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_idx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[0m_lazy_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mset_device\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_setDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(1):\n",
    "    # when we explicitly specify a device within a with context the tensor is created on the specified device.\n",
    "    # since we have not specified a cuda device here, tensor a will be created on CPU.\n",
    "    a = torch.tensor([10.0, 20.0])\n",
    "    # here, we explicitly create it on cuda:0, so, it will not be in cuda:1 as we specifed by context manager\n",
    "    a0 = torch.tensor([10.0, 20.0], device=cuda0)\n",
    "    # Here it will be created on default device, which is actually the device we set using context manager, cuda:1,\n",
    "    # so, the default for the current with context is cuda:1\n",
    "    a1 = torch.tensor([10.0, 20.0], device=cuda)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9845467",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4168\\1635667287.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# a1 was originally on cuda0, b1 has been created on cuda1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcuda1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'a0' is not defined"
     ]
    }
   ],
   "source": [
    "# If you want to explicitly move a tensor from one GPU to another, you have to perform a copy using the cuda() function that\n",
    "# we saw earlier or the to() function that we are going to see now.\n",
    "\n",
    "# torch.Tensor.to() function is used to perform tensor data type or device conversion.\n",
    "\n",
    "# a1 was originally on cuda0, b1 has been created on cuda1.\n",
    "b1 = a0.to(device=cuda1)\n",
    "\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1078dc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 20.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([10., 20.], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's create an example for copying between CPU and GPU\n",
    "\n",
    "# tensor on CPU\n",
    "my_cpu_tensor = torch.tensor([10.0, 20.0])\n",
    "display(my_cpu_tensor)\n",
    "\n",
    "# create a copy on device cuda:0. cuda refers to cuda:0.\n",
    "my_gpu0_tensor= my_cpu_tensor.to(device=cuda)\n",
    "display(my_gpu0_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9aa314fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4168\\4245776899.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# sum_a = a + a0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msum_cpu_gpu0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_cpu_tensor\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmy_gpu0_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# What if we tried to perform an operation on tensors located on two different devices?\n",
    "# a is on cpu, a0 is in cuda:0, this will fail\n",
    "# sum_a = a + a0\n",
    "# a1 is on cuda:1, a0 is on cuda:0, this will fail as well\n",
    "# sum_a = a1 + a0\n",
    "# this should work since a1 and x1 are in cuda:1\n",
    "# the resulting tensor will be on the same device as the original tensors on which the operation is was performed\n",
    "# sum_ax = a1 + x1\n",
    "\n",
    "sum_cpu_gpu0 = my_cpu_tensor + my_gpu0_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fee840e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61114dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2097152"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "543e6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache() will allow you to free up all of the unused cache memory within your caching allocator.\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "983a0405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2097152"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if the cache memory shows the same value, this means we do not have any unused memory in our cache.\n",
    "torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a933a119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 20.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.1000, 1.1000],\n",
       "        [1.1000, 1.1000]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If you want to create a new tensor on the same device, you can use PyTorch operations prefixed with the new_ .\n",
    "# Operations prefixed with \"new_\" create new tensors of the same type on the same device.\n",
    "\n",
    "display(x)\n",
    "\n",
    "# create a new 2x2 tensor based on x, x's data type and x's device, and fill the tensor with 1.1\n",
    "# x was on CPU, the new tensor will be on cpu as well\n",
    "preserve_context = x.new_full([2, 2], fill_value=1.1)\n",
    "display(preserve_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b10f1db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 20.], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.1000, 1.1000],\n",
       "        [1.1000, 1.1000]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now let's do it for cuda:0 device\n",
    "display(x0)\n",
    "\n",
    "# create a new tensor using x0, x0's data type and x0's device.\n",
    "# x0 was on cuda:0, the new tensor will on the cuda:0 as well\n",
    "preserve_context0 = x.new_full([2, 2], fill_value=1.1)\n",
    "display(preserve_context0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0067c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
