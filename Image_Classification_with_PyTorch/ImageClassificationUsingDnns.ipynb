{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7b77a1-4ad5-4144-8993-7985fcab714a",
   "metadata": {},
   "source": [
    "# Demo: Loading and Processing MNIST Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "081a8f88-0db1-423c-bc5b-936d94b94dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c8b154bd-875f-46e8-8e69-5da8a5478d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = pd.read_csv('datasets/mnist-in-csv/mnist_train.csv')\n",
    "mnist_test = pd.read_csv('datasets/mnist-in-csv/mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "43647fdb-4ad6-4f2a-8bee-3e9cb04ec89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 785), (10000, 785))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 60, 000 images in the training data,\n",
    "# and 10, 000 images in the test data.\n",
    "# Observe that the second dimension is 785.\n",
    "mnist_train.shape, mnist_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d6e1176c-d3e2-45df-ab24-f366c24ff222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm going to invoke the head function on these data frames so that we can explore this image data.\n",
    "# Observe that the individual columns in this csv file correspond to intensity values of pixels,\n",
    "# and observe that the very first column here is the label column. This corresponds to the numeric digit represented by those pixels, \n",
    "# 0 through 9 (namely, classes)\n",
    "mnist_train.head()\n",
    "\n",
    "\n",
    "# Each of these images is a 28 x 28 image,\n",
    "# which means it has a total of 784 pixels,\n",
    "# And there is one additional column corresponding to the label associated\n",
    "# with an image that gives us 785 columns for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9832c587-6a31-4417-ac23-aa4c830304f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at one of these images.\n",
    "# I'm going to extract the row at index 1 from the training data,\n",
    "# and I'm going to drop the column corresponding to the label.\n",
    "\n",
    "img = mnist_train[1:2] # take the image at the row with index 1\n",
    "\n",
    "# drop the label column\n",
    "img = img.drop('label', axis=1) # axis=1 drop the entire column, column-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "349afdf6-2067-4c99-8578-a2dff7a66037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This row only has the pixel intensity values for one image here.\n",
    "# I'm going to extract it in the form of a numpy array by accessing the values attribute of our pandas row.\n",
    "\n",
    "img = img.values\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "498226fd-0350-4fdb-8926-9fd949f38e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you look at the shape of this numpy array, you can see that it's 1 x 784.\n",
    "# Let's reshape this to be a single-channel image with the shape 1, 28, 28.\n",
    "img = img.reshape(1, 28, 28)\n",
    "\n",
    "# Here is a representation of the single image with the channel dimension\n",
    "# first; height and width are the remaining two dimension.\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "53543c53-f2b3-4996-b2f7-c0a72089df7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The squeeze function in a numpy array allows you to quickly eliminate\n",
    "# those dimensions which have just single values.\n",
    "# So image.squeeze will remove the unwanted one-dimensional access, so that we have an image which is just 28 x 28,\n",
    "# and the first dimension has been squeezed.\n",
    "img = img.squeeze()\n",
    "\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d4cb73fc-c9ea-4e39-a02c-651ecb830c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x265cd0c9e10>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I'll now use matplotlib to display this handwritten digit,\n",
    "# and you can see that this is the digit 0.\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d0778cd4-af3b-4d15-a6d4-4655dcbed8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set this image data up and use it to feed into a deep neural network.\n",
    "# The dropna function in a pandas data frame will drop any records which has missing fields,\n",
    "# so we now have a clean data set in mnist_train and mnist_test.\n",
    "\n",
    "mnist_train = mnist_train.dropna()\n",
    "mnist_test = mnist_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0ec6b6af-99a0-4d1b-b7ea-fc4b621ad471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now extract the features and labels into separate data frames to feed into our ML model.\n",
    "# We have mnist_train features and mnist_target. The target contains only the label.\n",
    "# The features do not contain the label.\n",
    "mnist_train_features = mnist_train.drop('label', axis=1)\n",
    "mnist_train_target = mnist_train['label']\n",
    "\n",
    "# We'll do exactly the same thing for the test data as well.\n",
    "# We'll extract the features and labels into separate data frames.\n",
    "mnist_test_features = mnist_test.drop('label', axis=1)\n",
    "mnist_test_target = mnist_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2295affd-bf94-4aef-98ce-b9abe4dd96d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train max-- 255\n",
      "train min-- 0\n",
      "test max-- 255\n",
      "test min-- 0\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at how this pixel intensity data is stored in this data frame.\n",
    "# Let's calculate the max and min pixel values for the\n",
    "# training as well as the test data,\n",
    "# and you can see that the max value is 255 and the min value is 0.\n",
    "# You can also see that these are integer values representing intensity.\n",
    "print(\"train max--\", mnist_train.values.max())\n",
    "print(\"train min--\", mnist_train.values.min())\n",
    "print(\"test max--\", mnist_test.values.max())\n",
    "print(\"test min--\", mnist_test.values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a205ba3f-ec49-4573-a819-e44736b48473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's been empirically shown that machine learning models such as neural network,\n",
    "# train and perform better when they're dealing with smaller numbers,\n",
    "# smaller floating-point numbers, as opposed to large values.\n",
    "# Let's convert all of our data to float32 and divide all of our data by 255\n",
    "# so pixel intensity values are expressed in the range 0-1 (since min value is 0, it becomes min-max normalization),\n",
    "# and we'll do the exact same thing for the test data as well.\n",
    "minst_train = mnist_train.astype('float32')\n",
    "mnist_train = mnist_train / 255\n",
    "\n",
    "mnist_test = mnist_test.astype('float32')\n",
    "mnist_test = mnist_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "95272186-1b17-455c-b8be-6f8ff8d9639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train max-- 1.0\n",
      "train min-- 0.0\n",
      "test max-- 1.0\n",
      "test min-- 0.0\n"
     ]
    }
   ],
   "source": [
    "# If you now calculate the min and max values of pixel intensity\n",
    "# on the training as well as the test data,\n",
    "# you can see that they now lie in the range 0-1.\n",
    "# And they're all floating-point values.\n",
    "print(\"train max--\", mnist_train.values.max())\n",
    "print(\"train min--\", mnist_train.values.min())\n",
    "print(\"test max--\", mnist_test.values.max())\n",
    "print(\"test min--\", mnist_test.values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "59c4bb24-a66e-453d-8661-4cb2c533897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With our image data set up in the way that we want to,\n",
    "# let's import the torch library.\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "29616ea1-2da5-40b3-a6ac-b196790be556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mnist_train.shape)\n",
    "display(mnist_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "84d29ffc-5cc4-4fc3-8b5f-6ffd80c1c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first thing that we'll do is to convert our image data, training as well as test data,\n",
    "# to torch tensors by invoking the torch.tensor function.\n",
    "# Observe that the data type for these images is torch.float,\n",
    "# and we convert the 'y' data, or our labels, to torch tensors, as well as of type torch.long.\n",
    "\n",
    "# Remember that these 'y' values are 0 through 9 integers which represent what digit is shown in the image (classes).\n",
    "\n",
    "X_train_tensor = torch.tensor(mnist_train_features.values, dtype=torch.float)\n",
    "x_test_tensor = torch.tensor(mnist_test_features.values, dtype=torch.float)\n",
    "\n",
    "Y_train_tensor = torch.tensor(mnist_train_target.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(mnist_test_target.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "22193275-4ddc-44fe-9f25-e77cfbb0748d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]), torch.Size([60000]))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the shape of these tensors.\n",
    "# Here are our training features and the corresponding training labels.\n",
    "# There are 60000 images, and each image has 784 pixels,\n",
    "# and there are 60000 corresponding labels as well.\n",
    "\n",
    "X_train_tensor.shape, Y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "90d936d4-03de-4f36-8dac-a730a3c2b0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 784]), torch.Size([10000]))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When we feed in images to a deep fully-connected (FC) neural network,\n",
    "# we'll feed them in as one-dimensional vectors.\n",
    "# Deep FC neural networks do not accept images in the original\n",
    "# two-dimensional format with height and width.\n",
    "\n",
    "\n",
    "# Let's take a look at our test data, there are 10000 test images that we'll use to evaluate our model.\n",
    "x_test_tensor.shape, y_test_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33fa483-8d03-4bc0-bbe5-123cad05add2",
   "metadata": {},
   "source": [
    "# Demo: Setting up a Fully Connected Neural Network for Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "482f35ee-5f9d-49d7-a802-bc0ff3247404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're now finally ready to design our deep neural network,\n",
    "# and for this we'll import the nn module from torch.nn.\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "03879397-8eae-4167-ab05-b37cfde16b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the input that we'll feed into our neural network is equal to 784,\n",
    "# corresponding to each of the pixels in every input image.\n",
    "# The input layer has to be connected to every pixel in the input image,\n",
    "# and this is a primary reason for parameter explosion.\n",
    "# Twenty-eight by 28 images are okay, but what if you had a 100 x 100 image?\n",
    "# Those are 10000 connections right there.\n",
    "input_size = 784\n",
    "\n",
    "# The output size is 10.\n",
    "# Each image can be any digit from 0 through 9. Those are ten categories.\n",
    "output_size = 10\n",
    "\n",
    "\n",
    "# We'll construct a deep neural network with two hidden layers,\n",
    "# one which has 16 neurons and another that has 32 neurons.\n",
    "hidden1_size = 16\n",
    "hidden2_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "dacd06c4-fb7e-4872-8989-46f5d246e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll instantiate a Net class that inherits from nn.module.\n",
    "# This will allow us to set up a custom neural network and\n",
    "# apply the activation functions that we want.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Here are the three fully connected dense layers in our neural\n",
    "        # network instantiated using the linear module.\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # You can see that the input size to the first input layer is 784,\n",
    "        # corresponding to the size of our images.\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        \n",
    "        # You can see that the output size corresponds to the\n",
    "        # final output of our neural network, which is equal to 10,\n",
    "        # corresponding to each of the 10 digits in into which\n",
    "        # the input images can be classified.\n",
    "        self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "    \n",
    "    \n",
    "    # If you remember,\n",
    "    # the output of any classification model is in the form of\n",
    "    # probability scores corresponding to probability that an\n",
    "    # image is one of these ten digits.\n",
    "    # The predicted label of our model is, of course,\n",
    "    # the label with the highest probability.\n",
    "    # That is the digit that our model considers to be the most\n",
    "    # likely to be represented by this input image.\n",
    "    \n",
    "    \n",
    "    # This forward function is what will be called in the\n",
    "    # forward pass of our neural network,\n",
    "    # which passes the input through all of the layers of our neural network.\n",
    "    def forward(self, x):\n",
    "        # We've applied the sigmoid activation function to our first two linear layers,\n",
    "        # fc1 and fc2.\n",
    "        # The sigmoid activation is simply an S curve.\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # The output of the first layer is passed to the second,\n",
    "        # the output of the second layer is passed to the third fully connected layer,\n",
    "        # and this third fully connected layer is the final layer of our neural\n",
    "        # network to which we apply the log softmax activation.\n",
    "        # Hopefully you're already familiar with the softmax\n",
    "        # function that we use with classifiers.\n",
    "        # Log softmax is equivalent to applying the log operation to the softmax function,\n",
    "        # and log softmax is often used with the NLLLoss function.\n",
    "        # NLLLoss stands for negative log likelihood loss,\n",
    "        # which is commonly used with PyTorch classification algorithms.\n",
    "        # Now classifiers use the cross entropy as a loss function.\n",
    "        # NLLLoss in PyTorch is more stable numerically and has better performance,\n",
    "        # so we use the NLLLoss along with log softmax for classification.\n",
    "        return torch.log_softmax(x, dim=-1) # -1 means last dimension, which is 784 in (60000, 784)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6bff76a7-711f-4abd-8a32-21c19a9bdd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an instance of this neural network model that we just designed.\n",
    "# model is equal to Net, and we want to train this model on a GPU if one is available.\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "24e891b3-0575-4014-91c5-2fc870279a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Quadro RTX 5000 with Max-Q Design\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# We'll use the torch.device device manager for this.\n",
    "# We want to use cuda:0 if cuda is available, otherwise we'll use the CPU.\n",
    "# If you take a look at the device, you'll see that it's cuda:0,\n",
    "# the single GPU that we had configured on this virtual machine.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "303d0a28-cd4a-47f1-9594-d33ca4e80279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cuda_stats():\n",
    "    #Additional Info when using cuda\n",
    "    if device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4c5bdd08-4076-4643-b8f3-c118765ae419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to train our model using this GPU,\n",
    "# we need to copy our model parameters over to this device\n",
    "# which we'll do using the model.to function.\n",
    "# Our model parameters are now on the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f9fd5f4e-e15e-4098-be18-6f7c7a6b9d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro RTX 5000 with Max-Q Design\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.2 GB\n"
     ]
    }
   ],
   "source": [
    "get_cuda_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6c1214c0-46cd-4044-b3e7-de097f6c9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now need to move our training and test tensors along with\n",
    "# the corresponding labels to the GPU as well.\n",
    "# Once again, we use the to() function to move all of our tensors to cuda:0.\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "x_test_tensor = x_test_tensor.to(device)\n",
    "\n",
    "Y_train_tensor = Y_train_tensor.to(device)\n",
    "y_test_tensor = y_test_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4923aea7-3e24-4c33-a131-a8f0a4b15d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro RTX 5000 with Max-Q Design\n",
      "Memory Usage:\n",
      "Allocated: 0.2 GB\n",
      "Cached:    0.2 GB\n"
     ]
    }
   ],
   "source": [
    "get_cuda_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "002df9ba-d862-4e96-806c-24a233c00265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go ahead and import the optim module from the PyTorch library.\n",
    "# This is the module that contains all of the optimizers that we can use to train our model.\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c364db30-ddf9-49c8-8a2d-1901a4f0702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are, of course, several optimizers that you can choose from.\n",
    "# I'm going to select the Adam optimizer, which is a computationally efficient optimization\n",
    "# algorithm with low memory requirements, which works well with large datasets,\n",
    "# and it's very popular and commonly used.\n",
    "# We haven't specified a learning rate for this optimizer,\n",
    "# it's 0.001 by default, and we use the default learning rate.\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# The loss function that we'll use to train our model is the NLLLoss,\n",
    "# that is the negative log likelihood loss.\n",
    "# This is what we use in combination with the log softmax output layer.\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2300b51e-6149-4a11-9de6-9231092d6fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now run training for a total of 500 epochs,\n",
    "# that is 500 passes through our dataset.\n",
    "# This is, of course, something that you can tweak.\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8c62cc49-14e9-4ce8-867a-a148bee64800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -- 10, loss -- 2.29 \n",
      "Epoch -- 20, loss -- 2.24 \n",
      "Epoch -- 30, loss -- 2.21 \n",
      "Epoch -- 40, loss -- 2.18 \n",
      "Epoch -- 50, loss -- 2.14 \n",
      "Epoch -- 60, loss -- 2.11 \n",
      "Epoch -- 70, loss -- 2.07 \n",
      "Epoch -- 80, loss -- 2.03 \n",
      "Epoch -- 90, loss -- 1.98 \n",
      "Epoch -- 100, loss -- 1.94 \n",
      "Epoch -- 110, loss -- 1.89 \n",
      "Epoch -- 120, loss -- 1.84 \n",
      "Epoch -- 130, loss -- 1.79 \n",
      "Epoch -- 140, loss -- 1.74 \n",
      "Epoch -- 150, loss -- 1.69 \n",
      "Epoch -- 160, loss -- 1.64 \n",
      "Epoch -- 170, loss -- 1.59 \n",
      "Epoch -- 180, loss -- 1.54 \n",
      "Epoch -- 190, loss -- 1.49 \n",
      "Epoch -- 200, loss -- 1.45 \n",
      "Epoch -- 210, loss -- 1.40 \n",
      "Epoch -- 220, loss -- 1.36 \n",
      "Epoch -- 230, loss -- 1.31 \n",
      "Epoch -- 240, loss -- 1.27 \n",
      "Epoch -- 250, loss -- 1.23 \n",
      "Epoch -- 260, loss -- 1.19 \n",
      "Epoch -- 270, loss -- 1.15 \n",
      "Epoch -- 280, loss -- 1.11 \n",
      "Epoch -- 290, loss -- 1.08 \n",
      "Epoch -- 300, loss -- 1.04 \n",
      "Epoch -- 310, loss -- 1.01 \n",
      "Epoch -- 320, loss -- 0.98 \n",
      "Epoch -- 330, loss -- 0.95 \n",
      "Epoch -- 340, loss -- 0.91 \n",
      "Epoch -- 350, loss -- 0.89 \n",
      "Epoch -- 360, loss -- 0.86 \n",
      "Epoch -- 370, loss -- 0.83 \n",
      "Epoch -- 380, loss -- 0.81 \n",
      "Epoch -- 390, loss -- 0.78 \n",
      "Epoch -- 400, loss -- 0.76 \n",
      "Epoch -- 410, loss -- 0.74 \n",
      "Epoch -- 420, loss -- 0.72 \n",
      "Epoch -- 430, loss -- 0.71 \n",
      "Epoch -- 440, loss -- 0.68 \n",
      "Epoch -- 450, loss -- 0.67 \n",
      "Epoch -- 460, loss -- 0.65 \n",
      "Epoch -- 470, loss -- 0.64 \n",
      "Epoch -- 480, loss -- 0.62 \n",
      "Epoch -- 490, loss -- 0.61 \n",
      "Epoch -- 500, loss -- 0.59 \n"
     ]
    }
   ],
   "source": [
    "# Now let's set up a simple for loop to run training for each epoch in the 500.\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    # Zero out the gradients of the optimizer,\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    # and make a forward pass through our model.\n",
    "    # This forward pass through the model will give us the predictions\n",
    "    # from our model using its current model parameters.\n",
    "    # We store this in Y_pred.\n",
    "    Y_pred = model(X_train_tensor) # do a forward pass\n",
    "    \n",
    "    \n",
    "    # We then apply the loss function to calculate the NLLLoss versus actual labels,\n",
    "    # Y_pred versus Y_train_tensor.\n",
    "    loss = loss_fn(Y_pred, Y_train_tensor) # calculate loss\n",
    "    \n",
    "    \n",
    "    # We then make a backward pass through this model by calling loss.backward.\n",
    "    # This backward pass is what will compute gradients.\n",
    "    loss.backward() # do backward pass\n",
    "    \n",
    "    \n",
    "    # Once gradients have been calculated,\n",
    "    # the optimizer.step function will update the model\n",
    "    # parameters using the learning rate, that is 0.001 by default.\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # In order to get visual feedback for training our model,\n",
    "    # for each epoch we'll print out the loss calculated.\n",
    "    if epoch  % 10 == 0: # print when epoch is divisable by 10\n",
    "        print('Epoch -- %d, loss -- %0.2f ' %(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "50a0c9da-3d85-4724-95e0-c1d3ab9823e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see that early on the loss value is around 2.27,\n",
    "# but as your model trains the loss value falls until\n",
    "# at the very end it's about 0.63."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "50394da1-6577-4cbe-8350-cf53dff7923a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once we have our fully trained model, let's use it for prediction.\n",
    "# Let's evaluate our model for this.\n",
    "# We need to switch over our model to the eval mode.\n",
    "# The eval mode ensures that layers and operations that should be applied only in training mode,\n",
    "# such as dropout and regularization, will be turned off in eval or prediction mode.\n",
    "# We don't have any such layers.\n",
    "# We anyway switch the model over to the eval mode\n",
    "# because that's the right thing to do.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bcd4a7ec-fbd3-47bf-9c5e-dbbbd9df42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a classifier model, the metrics used to evaluate the model is accuracy,\n",
    "# precision, and recall, and we'll import these functions from the sklearn library.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d3b5fb20-0202-4f92-9654-3a1d21c910c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([10000, 10])\n",
      "Accuracy:  0.863\n",
      "Precision:  0.8659877470998825\n",
      "Recall:  0.863\n"
     ]
    }
   ],
   "source": [
    "# When we use a model for prediction, we turn off gradient calculations,\n",
    "# we enclose all of our code within a torch.no_grad block.\n",
    "with torch.no_grad():\n",
    "    # We'll initialize two variables that'll keep track of the total number of\n",
    "    # correct predictions and the total number of predictions.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # We then feed all of the 10000 images in our test tensor to our model,\n",
    "    # and store the output of the model in the outputs variable.\n",
    "    outputs = model(x_test_tensor)\n",
    "    print(type(outputs), outputs.shape)\n",
    "    \n",
    "    # In order to get the predictions of our model,\n",
    "    # we need to find the maximum probability score from our output.\n",
    "    # https://pytorch.org/docs/stable/generated/torch.max.html?highlight=torch+max#torch.max\n",
    "    # second element of the tuple will be indices of max values over the dimension 1, in this case, they\n",
    "    # will correspond to labels, since our labels are also from 0 to 9.\n",
    "    # That will give us the predicted label for each input image in the test data.\n",
    "    _, predicted = torch.max(outputs.data, dim=1)\n",
    "    \n",
    "    \n",
    "    # Our test tensor and the model parameters are all on the GPU.\n",
    "    # I now get the actual labels from our y_test_tensor in the numpy format,\n",
    "    # and I get the predicted labels from our model onto the\n",
    "    # CPU before I calculate the accuracy, precision, and recall scores.\n",
    "    y_test = y_test_tensor.cpu().numpy()\n",
    "    predicted = predicted.cpu()\n",
    "    \n",
    "    \n",
    "    # Accuracy, precision, and recall are calculated using the actual labels from the test\n",
    "    # data versus the predicted labels from our model.\n",
    "    # This classification model performs multi-class classification,\n",
    "    # that is the input image can belong to one of ten categories.\n",
    "    # To calculate precision and recall scores for multi-class calculation,\n",
    "    # you need an averaging method,\n",
    "    # and the weighted method basically averages these scores based on the number\n",
    "    # of samples of the different categories in your test data.\n",
    "    print(\"Accuracy: \", accuracy_score(predicted, y_test))\n",
    "    print(\"Precision: \", precision_score(predicted, y_test, average='weighted'))\n",
    "    print(\"Recall: \", recall_score(predicted, y_test, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fb281801-84ee-4ab8-a9fa-2cd6ea363d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And you can see from the results here that accuracy,\n",
    "# precision, and recall scores are all around 85%.\n",
    "# This simple, fully connected neural network worked well when we had a simple image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e158f990-963b-4a0c-b082-08b0204c851c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
