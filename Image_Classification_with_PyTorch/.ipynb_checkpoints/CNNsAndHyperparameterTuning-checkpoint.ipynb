{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b971c6-7263-4b6a-b255-fb0218d2b94b",
   "metadata": {},
   "source": [
    "# Demo: Preparing and Exploring Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d0999ee3-2252-4b7a-a573-be9682f1b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we understand how convolutional layers and pooling layers work,\n",
    "# let's set up our first convolutional neural network to classify images.\n",
    "# We'll work with the MNIST dataset that we've seen before.\n",
    "# We'll work on  this notebook where we first set up our convolutional neural network.\n",
    "# We'll then perform some hyperparameter tuning to see how the model changes as we change its design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "05660f59-9223-4c70-9a9f-274834c6771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the import statements for the PyTorch and other data science libraries that we might need.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9c82d13a-8d09-48f3-b435-bbae054a5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the MNIST data that we've worked with before.\n",
    "# This data is available in the form of CSV files.\n",
    "\n",
    "mnist_train = pd.read_csv('datasets/mnist-in-csv/mnist_train.csv')\n",
    "mnist_test = pd.read_csv('datasets/mnist-in-csv/mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c99282b1-8051-462d-929e-682fbbf118e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you remember, these are grayscale 28 x 28 images.\n",
    "# Every row in this dataset corresponds to a single image along with its label.\n",
    "\n",
    "mnist_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0e644a7e-be26-46ec-bf7c-64e8d3fbb8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mnist_train.shape)\n",
    "display(mnist_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "de0819f9-699a-4a8e-b9c1-83e7a6ce53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll clean up our data first.\n",
    "# Let's drop all records which we have missing fields using dropna.\n",
    "\n",
    "mnist_train = mnist_train.dropna()\n",
    "mnist_test = mnist_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3e126d81-71c0-4713-b3f6-878c0b21289a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mnist_train.shape)\n",
    "display(mnist_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9da1a1d2-8549-4f05-92a3-5537293368b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 785)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We've seen these images before,\n",
    "# but let's just explore our data before we work with it.\n",
    "# I'm going to choose eight images at random to display and view.\n",
    "# These are eight records from our CSV file,\n",
    "# and you can see that each record has 785 columns.\n",
    "\n",
    "random_sel = mnist_train.sample(8)\n",
    "random_sel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f59bfc8e-efd5-4ae9-8e0d-40ebf53d027e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 28, 28])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm going to go ahead and drop the label column because we're only\n",
    "# interested in the image features of this batch.\n",
    "\n",
    "image_features = random_sel.drop('label', axis=1) # drop 'label' column\n",
    "\n",
    "# If you remember, the pixel intensity values were represented using integers from 0 to 255.\n",
    "# I'm going to convert this to a 0-1 scale by dividing every pixel by 255 (min-max normalization)\n",
    "# and that'll convert the result to a torch.Tensor.\n",
    "\n",
    "\n",
    "# In addition, I've also reshaped the images to be 28 x 28 images so that they can be displayed in matplotlib.\n",
    "# One shape dimension can be -1. In this case, the value is inferred from the length of the array and \n",
    "# remaining dimensions.\n",
    "image_batch = (torch.Tensor(image_features.values / 255.)).reshape(-1, 28, 28)\n",
    "\n",
    "# We have eight images in our batch here, and every image is 28 x 28 pixels.\n",
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c407df3c-05cb-4b8d-9611-75e5e6ccf02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 242])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's make a grid using torchvision.utils.make_grid(), and here is our grid shape.\n",
    "# .unsqueeze(1) is used to add an additional dimension, which is generally used for the channel. \n",
    "# Since the images are grayscale, they don't have a channel dimension, and .unsqueeze(1) adds that. \n",
    "# After this operation, the shape of new object representing batch of images (image_batch.unsqueeze(1)) \n",
    "# becomes [8, 1, 28, 28].\n",
    "# nrow=8 specifies that there should be 8 images in each row of the grid.\n",
    "grid = torchvision.utils.make_grid(image_batch.unsqueeze(1), nrow=8)\n",
    "\n",
    "# Observe that the make_grid utility converts our images to multi-channel or three-channel RGB images.\n",
    "# Now we are just doing it for this batch of eight images so that we can \n",
    "# display it because we've used the make_grid utility.\n",
    "# When we work with the images to feed into our CNN,\n",
    "# we'll work with them as single-channel grayscale images.\n",
    "grid.shape\n",
    "\n",
    "\n",
    "# The output tensor `grid` has a shape of `[3, 32, 242]` due to the default behavior of \n",
    "# `torchvision.utils.make_grid`:\n",
    "\n",
    "# 1. **Number of Channels**: The first dimension is 3 because `make_grid` by default converts \n",
    "# the images to have 3 channels (RGB) to create the grid image. \n",
    "# Even though the input images are grayscale (1 channel), the grid is created as an RGB image.\n",
    "\n",
    "# 2. **Height**: The second dimension is 32 because each input image has a height of 28, and \n",
    "# there is a padding of 2 pixels added by default to both top and bottom of each image. \n",
    "# So, 28 + 2 (top padding) + 2 (bottom padding) = 32.\n",
    "\n",
    "# 3. **Width**: The third dimension is 242 because each image has a width of 28, \n",
    "# there are 8 images in a row (`nrow=8`), and by default, \n",
    "# there is a padding of 2 pixels between each image as well as at the beginning and end of the row. \n",
    "# So, (28 * 8) + 2 (beginning padding) + 2 * 7 (padding between images) + 2 (end padding) = 242.\n",
    "\n",
    "# This output tensor represents an RGB image of the grid. \n",
    "# The height and width are slightly more than the dimensions of the \n",
    "# individual images due to the padding added between the images and on the borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d24d3168-ac0d-4521-8c95-52b4bec99657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 241.5, 31.5, -0.5)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now let's move the channel information to be the last dimension by using the transpose method.\n",
    "# This is because matplotlib expects our images to have\n",
    "# channel information in the last dimension.\n",
    "# Let's go ahead and take a look at the images that we're going to work with.\n",
    "# Here are eight images from our sample.\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1f98780c-85ca-4ba0-bc16-68b150cdb27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next few operations that I'm about to perform are familiar to us,\n",
    "# so I'm going to run through them quickly.\n",
    "# We extract the training features, that is the image pixel intensities and the\n",
    "# corresponding labels into separate tensors.\n",
    "# We do this for both the training data, as well as the test data.\n",
    "\n",
    "mnist_train_features = mnist_train.drop('label', axis=1)\n",
    "mnist_train_target = mnist_train['label']\n",
    "\n",
    "mnist_test_features = mnist_test.drop('label', axis=1)\n",
    "mnist_test_target = mnist_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9fd59f4a-8380-4e2d-b5b3-7e1f73895642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then convert all of the image data and the corresponding labels to torch.tensors.\n",
    "# Images are of type float, the labels are of type long.\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(mnist_train_features.values, dtype=torch.float)\n",
    "Y_train_tensor = torch.tensor(mnist_train_target.values, dtype=torch.long)\n",
    "\n",
    "x_test_tensor = torch.tensor(mnist_test_features.values, dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(mnist_test_target.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5e20879f-33e8-4824-8a91-0d053862f829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 784])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# Before we move on, let's quickly print out the shape of the training and test image data,\n",
    "# as well as the corresponding labels.\n",
    "# There are 60000 images in the training data and 10000 in the test data.\n",
    "print(X_train_tensor.shape)\n",
    "print(Y_train_tensor.shape)\n",
    "print(x_test_tensor.shape)\n",
    "print(y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "865b0c49-cbba-4733-bc3c-abfe1fae7deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# Remember that convolutional neural networks are\n",
    "# built to mimic the visual cortex,\n",
    "# which means it views images in the original two dimensions; height and width.\n",
    "# In addition, CNNs in PyTorch expect images to be fed in batches,\n",
    "# and also expect channel information to be one of the dimensions.\n",
    "# This is why we need to reshape the images in our training and\n",
    "# test data to be in four-dimensional form.\n",
    "\n",
    "X_train_tensor = X_train_tensor.reshape(-1, 1, 28, 28)\n",
    "x_test_tensor = x_test_tensor.reshape(-1, 1, 28, 28)\n",
    "\n",
    "# The first dimension is the batch. Here the first dimension is -1 (it means - inferred from the other dimensions)\n",
    "# It'll correspond to the number of images that we have to work with,\n",
    "# and the remaining three dimensions refer to a single image.\n",
    "# Each image is a 28 x 28 grayscale image.\n",
    "# Let's take a look at the shape of our training and test data after we've performed this reshape.\n",
    "# You can see that we have 60000 images, and each image is 28 x 28 x 1.\n",
    "# This is in the training data, \n",
    "# and we have 10000 images in the same format in the test data.\n",
    "print(X_train_tensor.shape)\n",
    "print(Y_train_tensor.shape)\n",
    "print(x_test_tensor.shape)\n",
    "print(y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f71dd-f248-4d19-9f86-70a653fa524d",
   "metadata": {},
   "source": [
    "# Demo: Setting up a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0ac9ae65-86d2-4c2b-baa9-a3dc6e2bbdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now ready to set up our convolutional neural network.\n",
    "# Go ahead and import the PyTorch modules that we'll need.\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "98576827-cc4b-4dc1-b043-200d6a90ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll set up a few variables which holds the different size information in our convolutional neural network.\n",
    "# Let's look at each of these in turn.\n",
    "\n",
    "# in_size here refers to the number of channels in your input image.\n",
    "# The MNIST dataset is made up of grayscale images, which is why in_size is 1 here.\n",
    "# If you're working with RGB images, in_size will be 3.\n",
    "in_size = 1\n",
    "\n",
    "# We'll have two convolutional layers in our neural network.\n",
    "# The first of these will be of size 16, and the second of size 32.\n",
    "# These refer to the number of feature maps generated by each convolutional layer,\n",
    "# that is the depth of the output produced by each convolutional layer.\n",
    "hid1_size = 16\n",
    "hid2_size = 32\n",
    "\n",
    "# The output size here refers to the size of the output\n",
    "# from the final linear layer of our CNN.\n",
    "# Because we have 10 digits into which the input images can be classified,\n",
    "# this is equal to 10.\n",
    "out_size = 10\n",
    "\n",
    "# This is the convolutional kernel size.\n",
    "# Both of our convolutional layers will use a square kernel that is 5 x 5.\n",
    "k_conv_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "652e1bcc-0921-48e2-b9af-e976f6ff49a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up our custom neural network,\n",
    "# which inherits from the nn.Module base class.\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    # Let's first set up the init method where we initialize the\n",
    "    # convolutional and pooling layers of our neural network.\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # Observe that we have set up two sequential layers.\n",
    "        # Each sequential layer is made up of a convolutional layer,\n",
    "        # batch normalization, ReLU activation, followed by a MaxPool layer.\n",
    "        self.layer1 = nn.Sequential(\n",
    "                                   # Here is the first convolutional layer where the input size is\n",
    "                                   # the number of channels in the input images.\n",
    "                                   # hid1_size refers to the depth of the output of this convolutional layer.\n",
    "                                   # This is equal to 16 for this first convolutional layer,\n",
    "                                   # and this convolutional layer will apply a 5 x 5 kernel to the input images.\n",
    "                                   # The default horizontal and vertical stride for the convolutional kernel is 1.\n",
    "                                   nn.Conv2d(in_size, hid1_size, k_conv_size),\n",
    "                                   # Batch normalization is a technique that is often used to improve the speed,\n",
    "                                   # performance, and stability of your neural networks.\n",
    "                                   # Batch normalization is used to 0 center and normalize the output of each\n",
    "                                   # convolutional layer before the activations that follow.\n",
    "                                   # This allows your model to learn the optimal scale and\n",
    "                                   # the mean of the inputs for each layer.\n",
    "                                   # Neural networks often work better and convert faster in training\n",
    "                                   # when the inputs to each layer are batch normalized,\n",
    "                                   # so we apply batch normalization to the output of our convolutional\n",
    "                                   # layer before we pass into the ReLU activation layer.\n",
    "                                   # OUTPUT HERE: (28-5 + 0) / 1 + 1 = 24. 24x24\n",
    "                                   nn.BatchNorm2d(hid1_size), # 24x24\n",
    "                                   nn.ReLU(), #  24x24\n",
    "                                   # And the output of this convolutional layer is passed into a MaxPool layer\n",
    "                                   # where we use a 2 x 2 kernel to subsample the input.\n",
    "                                   # The default horizontal and vertical stride for a pooling layer\n",
    "                                   # kernel is equal to the size of the kernel itself,\n",
    "                                   # which will be 2 here in our example.\n",
    "                                   nn.MaxPool2d(kernel_size=2)) # 2x2 pooling filter\n",
    "                                   # OUTPUT after pooling: (24 - 2 + 0) / 2 + 1 = 12. So 12x12.\n",
    "        \n",
    "        # After this first combination of a convolutional and a pooling layer,\n",
    "        # we pass it in to another convolutional and pooling layer.\n",
    "        # Remember that convolutional layers make your input deeper.\n",
    "        # The output of the second convolutional layer will have 32 feature maps.\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(hid1_size, hid2_size, k_conv_size),\n",
    "                                    # OUTPUT HERE: (12 - 5 + 0) / 1 + 1 = 8. So 8x8\n",
    "                                   nn.BatchNorm2d(hid2_size), # 8x8\n",
    "                                   nn.ReLU(), # 8x8\n",
    "                                   nn.MaxPool2d(kernel_size=2)) \n",
    "                                    # OUTPUT after pooling: (8 - 2 + 0) / 2 + 1 = 4. So 4 x 4. \n",
    "        \n",
    "        # After the two sets of layers performing convolution and pooling,\n",
    "        # we pass this output into a final linear layer.\n",
    "        # The size of this linear layer depends on the size of the\n",
    "        # outputs produced by our previous layers,\n",
    "        # and it's 512 here based on our input image size and the strides and the\n",
    "        # kernel size that we have chosen for our convolution,\n",
    "        # as well as pooling layers.\n",
    "        # 4 x 4 output image from the last layer, and 32 feature maps, which makes 4 x 4 x 32 = 512.\n",
    "        self.fc = nn.Linear(512, out_size)\n",
    "        \n",
    "        \n",
    "    # Let's set up the forward function for this neural network,\n",
    "    # which takes in the input images and applies the convolutional,\n",
    "    # pooling, and linear layers to the input.\n",
    "    def forward(self, x):\n",
    "        # We'll first apply the first layer to our input batch of images,\n",
    "        # and I'm going to print out the shape of the output so that I can\n",
    "        # see what the convolution result looks like.\n",
    "        out = self.layer1(x)\n",
    "        # print(out.shape)\n",
    "            \n",
    "        # The output of the first layer is then passed in to the second layer,\n",
    "        # and I print out the shape of the output which we get\n",
    "        # from the second layer as well.\n",
    "        out = self.layer2(out)\n",
    "        # print(out.shape)\n",
    "            \n",
    "        # Now remember the convolutional and pooling layers\n",
    "        # act on images in two dimensions, images that have height and width,\n",
    "        # but when we feed in this information to the linear layer,\n",
    "        # we'll need to reshape the output that we get from our\n",
    "        # convolutional and pooling layers.\n",
    "        # This reshape operation serves to flatten the images that we get in each batch so\n",
    "        # that every image is represented in the form of a 1D vector.\n",
    "        out = out.reshape(out.size(0), -1) # keep the batch dimension, flatten other dimensions into one dim\n",
    "        # I'll print out the shape of these flattened images as well so that\n",
    "        # you can see how the shape of the image changes as you flow through\n",
    "        # the convolutional neural network.\n",
    "        # print(out.shape)\n",
    "            \n",
    "            \n",
    "        # And this output will now be fed into our fully connected layer.\n",
    "        # Remember we had specified 512 pixels in our fully connected layer.\n",
    "        # By printing out these output shapes, you'll see how we got 512.\n",
    "        # So if you're not sure of how many pixels you should have in your linear layer,\n",
    "        # you can simply leave the linear layer out,\n",
    "        # print out the shapes at every layer of your convolutional neural network,\n",
    "        # and once you know the shape of the final output you\n",
    "        # can then set up your linear layer.\n",
    "        out = self.fc(out)\n",
    "        # print(out.shape)\n",
    "            \n",
    "            \n",
    "        # I'm going to train this model using cross entropy loss,\n",
    "        # which can directly take as input the output of your last linear layer.\n",
    "        # You can choose to apply the softmax function here if you want to,\n",
    "        # and that works as well,\n",
    "        # or you can simply use the output of this last linear layer.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "862c3df2-1529-4738-9f6d-e3a1530dd3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of this model,\n",
    "model = ConvNet()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# and because I want to train this model using the GPU available,\n",
    "# I'm going to copy over the model parameters to this coda:0 device\n",
    "# calling model.to(device) in order to perform this copy.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1861df6b-0fb2-450b-900e-1f5478090d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to the model parameters,\n",
    "# you also need to copy over the tensors for the training and\n",
    "# test images and the corresponding labels,\n",
    "# and we do this as well using the to() function.\n",
    "\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "x_test_tensor = x_test_tensor.to(device)\n",
    "\n",
    "Y_train_tensor = Y_train_tensor.to(device)\n",
    "y_test_tensor = y_test_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddac141-081e-463e-936a-c0e3c25f6519",
   "metadata": {},
   "source": [
    "# Demo: Training a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2fce123e-e928-468a-a702-d038ee6ebdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning_rate that we'll use to train our\n",
    "# convolutional neural network is 0.001,\n",
    "# and the loss function is the CrossEntropyLoss.\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "de9561bb-2f90-41a3-bff5-a43b26925e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We use the CrossEntropyLoss when we apply softmax activation to our linear layer,\n",
    "# or this can work directly with the output of the linear layer as well.\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ad83048c-f1cd-4432-83ad-e2ee4aee2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an Adam optimizer with our model parameters and\n",
    "# the learning rate that we have chosen.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "75eb1598-7e6a-4cf4-91ad-44fe84c7b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll train our convolutional neural network for a total of 10 epochs,\n",
    "# and we'll maintain the loss for each epoch in a list called loss_values.\n",
    "\n",
    "num_epochs = 10\n",
    "loss_values = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b7efbcaa-22db-43e9-b8b2-3d42b5d7788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cuda_stats():\n",
    "    #Additional Info when using cuda\n",
    "    if device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "24d4289a-697c-433a-9917-0d9cc45a0844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro RTX 5000 with Max-Q Design\n",
      "Memory Usage:\n",
      "Allocated: 0.2 GB\n",
      "Cached:    27.9 GB\n"
     ]
    }
   ],
   "source": [
    "get_cuda_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "822b5b1d-5f2f-449b-8949-3826ee54c40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1, loss - 2.52409 \n",
      "Epoch - 2, loss - 3.46788 \n",
      "Epoch - 3, loss - 2.58975 \n",
      "Epoch - 4, loss - 1.70555 \n",
      "Epoch - 5, loss - 1.55359 \n",
      "Epoch - 6, loss - 1.13371 \n",
      "Epoch - 7, loss - 1.09458 \n",
      "Epoch - 8, loss - 1.02838 \n",
      "Epoch - 9, loss - 0.78641 \n"
     ]
    }
   ],
   "source": [
    "# We find that the code to train this convolutional neural network is the same\n",
    "# as the fully connected neural network that we saw before.\n",
    "# We run a for loop for each epoch, calculate the output of the current model,\n",
    "# calculate the loss of the actual labels versus the\n",
    "# predicted labels from our model,\n",
    "# zero out the gradients for our model by calling optimizer.zero_grad,\n",
    "# make a backward pass for the neural network,\n",
    "# and call optimizer.step to update the model parameters.\n",
    "# I'll then print out and append the value of loss for each epoch.\n",
    "for epoch in range (1, num_epochs):\n",
    "    \n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, Y_train_tensor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step() # update model parameters\n",
    "    \n",
    "    print('Epoch - %d, loss - %0.5f '%(epoch, loss.item()))\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "\n",
    "\n",
    "# Let's start the training process, and thanks to all of the print statements that we put in\n",
    "# the forward pass of the neural network,\n",
    "# you can see how the shape of the data changes as it passes through your various layers.\n",
    "# You can see that the output of the first layer which contains a\n",
    "# convolutional layer as well as a pooling layer is the 60000 images that we had in our training data \n",
    "# and 16, 12, 12. So how did we get this?\n",
    "# The first 16 here refers to the depth of the convolutional layer output.\n",
    "# This convolutional layer outputs 16 feature maps.\n",
    "# The default padding on the input is 0,\n",
    "# that means we haven't added any additional padding pixels,\n",
    "# and the stride of the convolutional kernel is equal to 1 in both the\n",
    "# horizontal as well as the vertical directions.\n",
    "# Given that our input images were 28 x 28 images,\n",
    "# the size of the output convolutional result will be 24 x 24 images\n",
    "# thanks to our convolutional kernel size of 5.\n",
    "# When the output of the convolutional layer,\n",
    "# which are feature maps, 16 feature maps,\n",
    "# each of size 24 x 24, are passed into the MaxPool layer,\n",
    "# the output of the MaxPool layer will be 12 pixels by 12 pixels.\n",
    "# This is because the kernel size of our MaxPool layer is 2 and the stride is 2 by default, \n",
    "# both horizonatally and vertically. This will halve the size of the input fed into this MaxPool layer.\n",
    "\n",
    "\n",
    "# So 12 x 12 inputs will be fed into layer 2,\n",
    "# and the output of layer 2 will be 4 x 4.\n",
    "# The depth of the feature maps will change because the convolutional\n",
    "# layer in layer 2 will output 32 feature maps.\n",
    "# You can apply the same math that we did before to see how the 12 x 12\n",
    "# inputs were reduced to 4 x 4 outputs of this layer.\n",
    "\n",
    "\n",
    "# Now that we know the output of this layer 2,\n",
    "# you can now see how we calculated the size of the linear layer,\n",
    "# 32 multiplied by 4 multiplied by 4 gives us 512.\n",
    "# This fully connected linear layer has an output size of 10 corresponding\n",
    "# to the 10 digits that images can be classified into.\n",
    "# And this is an easy, practical way for you to calculate the shape of your output as it passes\n",
    "# through the layers that you've set up in your neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8fb7c03b-190b-4d8c-b32f-dbdb9e44e090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro RTX 5000 with Max-Q Design\n",
      "Memory Usage:\n",
      "Allocated: 0.2 GB\n",
      "Cached:    27.9 GB\n"
     ]
    }
   ],
   "source": [
    "get_cuda_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b021b913-fe2e-4317-bf00-6bf7c624bb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# As we train our neural network,\n",
    "# we store the loss values for each epoch in a list.\n",
    "#  Let's plot these loss values to see how the loss\n",
    "# moved through the training process.\n",
    "x = range(0, 9)\n",
    "\n",
    "# We use matplotlib for this,\n",
    "# and you can see that as we trained our model\n",
    "# further the loss value fell sharply.\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.plot(x, loss_values)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2510f2a5-b533-4a58-ba6b-8aada462d36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are now ready to see how this model performs on the test data.\n",
    "# Switch over to the eval mode.\n",
    "# Because we have batch normalization after our convolutional layers,\n",
    "# this eval mode is important for this particular model.\n",
    "# Batch normalization will be turned off during prediction.\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "dcb0150c-e897-4e28-8fd0-9f04f05764f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again, we'll evaluate our model by calculating the accuracy,\n",
    "# precision, and recall scores.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "be56c732-1f63-4c14-8f8e-1c61875247c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([10000, 10])\n",
      "Accuracy:  0.6789\n",
      "Precision:  0.8219186365403566\n",
      "Recall:  0.6789\n"
     ]
    }
   ],
   "source": [
    "# Now the code to feed in the test data to our model for\n",
    "# evaluation is exactly the same as we saw earlier when we built up our fully connected neural network.\n",
    "\n",
    "# When we use a model for prediction, we turn off gradient calculations,\n",
    "# we enclose all of our code within a torch.no_grad block.\n",
    "with torch.no_grad():\n",
    "    # We'll initialize two variables that'll keep track of the total number of\n",
    "    # correct predictions and the total number of predictions.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # We then feed all of the 10000 images in our test tensor to our model,\n",
    "    # and store the output of the model in the outputs variable.\n",
    "    outputs = model(x_test_tensor)\n",
    "    print(type(outputs), outputs.shape)\n",
    "    \n",
    "    # In order to get the predictions of our model,\n",
    "    # we need to find the maximum probability score from our output.\n",
    "    # https://pytorch.org/docs/stable/generated/torch.max.html?highlight=torch+max#torch.max\n",
    "    # second element of the tuple will be indices of max values over the dimension 1, in this case, they\n",
    "    # will correspond to labels, since our labels are also from 0 to 9.\n",
    "    # That will give us the predicted label for each input image in the test data.\n",
    "    _, predicted = torch.max(outputs.data, dim=1)\n",
    "    \n",
    "    \n",
    "    # Our test tensor and the model parameters are all on the GPU.\n",
    "    # I now get the actual labels from our y_test_tensor in the numpy format,\n",
    "    # and I get the predicted labels from our model onto the\n",
    "    # CPU before I calculate the accuracy, precision, and recall scores.\n",
    "    y_test = y_test_tensor.cpu().numpy()\n",
    "    predicted = predicted.cpu()\n",
    "    \n",
    "    \n",
    "    # Accuracy, precision, and recall are calculated using the actual labels from the test\n",
    "    # data versus the predicted labels from our model.\n",
    "    # This classification model performs multi-class classification,\n",
    "    # that is the input image can belong to one of ten categories.\n",
    "    # To calculate precision and recall scores for multi-class calculation,\n",
    "    # you need an averaging method,\n",
    "    # and the weighted method basically averages these scores based on the number\n",
    "    # of samples of the different categories in your test data.\n",
    "    print(\"Accuracy: \", accuracy_score(predicted, y_test))\n",
    "    print(\"Precision: \", precision_score(predicted, y_test, average='weighted'))\n",
    "    print(\"Recall: \", recall_score(predicted, y_test, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "34d82f4c-ff0f-41c3-bad2-1a1fa6ffd9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here is the Accuracy, Precision,\n",
    "# and Recall of our convolutional neural network.\n",
    "# You might feel that these values are lower,\n",
    "# but we haven't trained the model for very long.\n",
    "# You can increase the number of epochs and you'll find\n",
    "# that the model performs better.\n",
    "\n",
    "# And we can also perform hyperparameter tuning, which we'll do in the next demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde3fd8-c347-48ac-b6ab-6c590249b226",
   "metadata": {},
   "source": [
    "# Demo: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4750704e-3410-4a30-a809-5d7b25a08dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the real world,\n",
    "# you have to experiment with the different design parameters for your\n",
    "# neural network to see what works well in your data.\n",
    "# This process is called hyperparameter tuning,\n",
    "# and often deep learning frameworks running on cloud platforms\n",
    "# offer you an automated way of doing this.\n",
    "# Here we'll change these parameters manually,\n",
    "# rerun and retrain our model to see how it performs.\n",
    "\n",
    "\n",
    "in_size = 1\n",
    "\n",
    "# I'll first change the number of feature maps that our first\n",
    "# convolutional layer outputs from 16 down to 8.\n",
    "hid1_size = 8\n",
    "hid2_size = 32\n",
    "\n",
    "out_size = 10\n",
    "\n",
    "k_conv_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8c766e25-aade-4f3b-8514-5905c3380727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll work with the same design of the convolutional neural network. So, no change on ConvNet class.\n",
    "# Remember that I had printed out the shapes of the output layer after each layer.\n",
    "# I'm going to comment all of this out so that our output is clean.\n",
    "# All you need to do to execute and train this new model.\n",
    "# For hid1_size = 8, You can see that for the same number of epochs of training,\n",
    "# all of these metrics have fallen.\n",
    "# This is a worse model.\n",
    "# Clearly the additional feature maps that we had were\n",
    "# detecting important features in the input.\n",
    "# I'll now scroll back up on this notebook and reset the hid1_size back to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a3e7aca3-965f-453c-a993-373df1e8d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next change that I'll make here is not in the design\n",
    "# of my convolutional neural network, but to the learning_rate of my model.\n",
    "# I'll increase the learning_rate so that it learns more at every epoch.\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "# Let's scroll down to the very bottom, and you can see that the Accuracy,\n",
    "# Precision, and Recall of this model have increased thanks to the increased learning_rate.\n",
    "# When you design your neural network,\n",
    "# you don't know what design parameters work well\n",
    "# until you try it out on your data, and this is exactly what we're doing here.\n",
    "# This is our hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a482e595-11f6-4286-ab1a-bd69b320f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try one more thing here.\n",
    "# I'm going to switch the learning_rate back to 0.001,\n",
    "# and instead of CrossEntropyLoss I'm going to switch\n",
    "# this model over to use NLLLoss.\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2b33a259-5d15-4a6c-98c3-88122a2701b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I'll need to make another change to the design of my model.\n",
    "# Instead of the output linear layer,\n",
    "# I need to apply a log_softmax function to the output.\n",
    "# Remember, when you're using NLLLoss you have to have a log_softmax output.\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "                                   nn.Conv2d(in_size, hid1_size, k_conv_size),\n",
    "                                   # OUTPUT HERE: (28-5 + 0) / 1 + 1 = 24. 24x24\n",
    "                                   nn.BatchNorm2d(hid1_size), # 24x24\n",
    "                                   nn.ReLU(), #  24x24\n",
    "                                   nn.MaxPool2d(kernel_size=2)) # 2x2 pooling filter\n",
    "                                   # OUTPUT after pooling: (24 - 2 + 0) / 2 + 1 = 12. So 12x12.\n",
    "        \n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(hid1_size, hid2_size, k_conv_size),\n",
    "                                    # OUTPUT HERE: (12 - 5 + 0) / 1 + 1 = 8. So 8x8\n",
    "                                   nn.BatchNorm2d(hid2_size), # 8x8\n",
    "                                   nn.ReLU(), # 8x8\n",
    "                                   nn.MaxPool2d(kernel_size=2)) \n",
    "                                    # OUTPUT after pooling: (8 - 2 + 0) / 2 + 1 = 4. So 4 x 4. \n",
    "        \n",
    "        self.fc = nn.Linear(512, out_size)\n",
    "        \n",
    "        \n",
    "    # Let's set up the forward function for this neural network,\n",
    "    # which takes in the input images and applies the convolutional,\n",
    "    # pooling, and linear layers to the input.\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        # print(out.shape)\n",
    "            \n",
    "        out = self.layer2(out)\n",
    "        # print(out.shape)\n",
    "            \n",
    "        out = out.reshape(out.size(0), -1) # keep the batch dimension, flatten other dimensions into one dim\n",
    "        # print(out.shape)\n",
    "            \n",
    "            \n",
    "        out = self.fc(out)\n",
    "        # print(out.shape)\n",
    "            \n",
    "            \n",
    "        return F.log_softmax(out, dim=-1) # -1 means last dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d98aac8b-69ac-43d0-bf86-7ad04bca8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go ahead and choose the Run option and run all of the\n",
    "# cells and see how this model performs.\n",
    "# We'll run training for the same 10 epochs,\n",
    "# scroll down to the very bottom, and you can see that the Accuracy,\n",
    "# Precision, and Recall of this model has fallen a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fa3f9668-f26c-4a44-9646-387dbdd8f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are other things that we can change.\n",
    "# Before that, let's get back to the original state.\n",
    "# I'll choose CrossEntropyLoss, and this time,\n",
    "# instead of the Adam optimizer I'll choose another optimizer,\n",
    "# the SGD, or the Stochastic gradient descent optimizer with momentum.\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "\n",
    "# Now since we switched back to using the CrossEntropyLoss,\n",
    "# the output layer can no longer be a log_softmax layer,\n",
    "# so I'm going to go ahead and get rid of this and return\n",
    "# the output of the linear layer directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c69e866-a919-4c6d-989b-e325f00758c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I go down to the very bottom, once this model has been trained,\n",
    "# you can see that the Accuracy, Precision,\n",
    "# and Recall scores of this model have fallen terribly.\n",
    "\n",
    "# These are just a few examples of hyperparameters that you could do.\n",
    "# Let's go back to the Adam optimizer that worked well for us,\n",
    "# let's go back to a learning_rate of 0.01,\n",
    "# and let's go ahead and run all of the cells once again so that we\n",
    "# continue working with the best model that we've seen so far.\n",
    "# There will always be slight variations in your model each time you run training,\n",
    "# and you can see that the Accuracy, Precision,\n",
    "# and Recall of this model is 88% or above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
