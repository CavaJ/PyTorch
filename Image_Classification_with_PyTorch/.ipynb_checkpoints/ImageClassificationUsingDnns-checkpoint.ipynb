{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "081a8f88-0db1-423c-bc5b-936d94b94dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8b154bd-875f-46e8-8e69-5da8a5478d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = pd.read_csv('datasets/mnist-in-csv/mnist_train.csv')\n",
    "mnist_test = pd.read_csv('datasets/mnist-in-csv/mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43647fdb-4ad6-4f2a-8bee-3e9cb04ec89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 785), (10000, 785))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 60, 000 images in the training data,\n",
    "# and 10, 000 images in the test data.\n",
    "# Observe that the second dimension is 785.\n",
    "mnist_train.shape, mnist_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6e1176c-d3e2-45df-ab24-f366c24ff222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm going to invoke the head function on these data frames so that we can explore this image data.\n",
    "# Observe that the individual columns in this csv file correspond to intensity values of pixels,\n",
    "# and observe that the very first column here is the label column. This corresponds to the numeric digit represented by those pixels, \n",
    "# 0 through 9 (namely, classes)\n",
    "mnist_train.head()\n",
    "\n",
    "\n",
    "# Each of these images is a 28 x 28 image,\n",
    "# which means it has a total of 784 pixels,\n",
    "# And there is one additional column corresponding to the label associated\n",
    "# with an image that gives us 785 columns for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9832c587-6a31-4417-ac23-aa4c830304f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at one of these images.\n",
    "# I'm going to extract the row at index 1 from the training data,\n",
    "# and I'm going to drop the column corresponding to the label.\n",
    "\n",
    "img = mnist_train[1:2] # take the image at the row with index 1\n",
    "\n",
    "# drop the label column\n",
    "img = img.drop('label', axis=1) # axis=1 drop the entire column, column-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "349afdf6-2067-4c99-8578-a2dff7a66037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This row only has the pixel intensity values for one image here.\n",
    "# I'm going to extract it in the form of a numpy array by accessing the values attribute of our pandas row.\n",
    "\n",
    "img = img.values\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "498226fd-0350-4fdb-8926-9fd949f38e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you look at the shape of this numpy array, you can see that it's 1 x 784.\n",
    "# Let's reshape this to be a single-channel image with the shape 1, 28, 28.\n",
    "img = img.reshape(1, 28, 28)\n",
    "\n",
    "# Here is a representation of the single image with the channel dimension\n",
    "# first; height and width are the remaining two dimension.\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53543c53-f2b3-4996-b2f7-c0a72089df7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The squeeze function in a numpy array allows you to quickly eliminate\n",
    "# those dimensions which have just single values.\n",
    "# So image.squeeze will remove the unwanted one-dimensional access, so that we have an image which is just 28 x 28,\n",
    "# and the first dimension has been squeezed.\n",
    "img = img.squeeze()\n",
    "\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d4cb73fc-c9ea-4e39-a02c-651ecb830c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b983efe278>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I'll now use matplotlib to display this handwritten digit,\n",
    "# and you can see that this is the digit 0.\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d0778cd4-af3b-4d15-a6d4-4655dcbed8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set this image data up and use it to feed into a deep neural network.\n",
    "# The dropna function in a pandas data frame will drop any records which has missing fields,\n",
    "# so we now have a clean data set in mnist_train and mnist_test.\n",
    "\n",
    "mnist_train = mnist_train.dropna()\n",
    "mnist_test = mnist_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ec6b6af-99a0-4d1b-b7ea-fc4b621ad471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now extract the features and labels into separate data frames to feed into our ML model.\n",
    "# We have mnist_train features and mnist_target. The target contains only the label.\n",
    "# The features do not contain the label.\n",
    "mnist_train_features = mnist_train.drop('label', axis=1)\n",
    "mnist_train_target = minst_train['label']\n",
    "\n",
    "# We'll do exactly the same thing for the test data as well.\n",
    "# We'll extract the features and labels into separate data frames.\n",
    "mnist_test_features = mnist_test.drop('label', axis=1)\n",
    "mnist_test_target = mnist_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2295affd-bf94-4aef-98ce-b9abe4dd96d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train max-- 255\n",
      "train min-- 0\n",
      "test max-- 255\n",
      "test min-- 0\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at how this pixel intensity data is stored in this data frame.\n",
    "# Let's calculate the max and min pixel values for the\n",
    "# training as well as the test data,\n",
    "# and you can see that the max value is 255 and the min value is 0.\n",
    "# You can also see that these are integer values representing intensity.\n",
    "print(\"train max--\", mnist_train.values.max())\n",
    "print(\"train min--\", mnist_train.values.min())\n",
    "print(\"test max--\", mnist_test.values.max())\n",
    "print(\"test min--\", mnist_test.values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a205ba3f-ec49-4573-a819-e44736b48473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's been empirically shown that machine learning models such as neural network,\n",
    "# train and perform better when they're dealing with smaller numbers,\n",
    "# smaller floating-point numbers, as opposed to large values.\n",
    "# Let's convert all of our data to float32 and divide all of our data by 255\n",
    "# so pixel intensity values are expressed in the range 0-1 (since min value is 0, it becomes min-max normalization),\n",
    "# and we'll do the exact same thing for the test data as well.\n",
    "minst_train = mnist_train.astype('float32')\n",
    "mnist_train = mnist_train / 255\n",
    "\n",
    "mnist_test = mnist_test.astype('float32')\n",
    "mnist_test = mnist_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95272186-1b17-455c-b8be-6f8ff8d9639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train max-- 1.0\n",
      "train min-- 0.0\n",
      "test max-- 1.0\n",
      "test min-- 0.0\n"
     ]
    }
   ],
   "source": [
    "# If you now calculate the min and max values of pixel intensity\n",
    "# on the training as well as the test data,\n",
    "# you can see that they now lie in the range 0-1.\n",
    "# And they're all floating-point values.\n",
    "print(\"train max--\", mnist_train.values.max())\n",
    "print(\"train min--\", mnist_train.values.min())\n",
    "print(\"test max--\", mnist_test.values.max())\n",
    "print(\"test min--\", mnist_test.values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59c4bb24-a66e-453d-8661-4cb2c533897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With our image data set up in the way that we want to,\n",
    "# let's import the torch library.\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29616ea1-2da5-40b3-a6ac-b196790be556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mnist_train.shape)\n",
    "display(mnist_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "84d29ffc-5cc4-4fc3-8b5f-6ffd80c1c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first thing that we'll do is to convert our image data, training as well as test data,\n",
    "# to torch tensors by invoking the torch.tensor function.\n",
    "# Observe that the data type for these images is torch.float,\n",
    "# and we convert the 'y' data, or our labels, to torch tensors, as well as of type torch.long.\n",
    "\n",
    "# Remember that these 'y' values are 0 through 9 integers which represent what digit is shown in the image (classes).\n",
    "\n",
    "X_train_tensor = torch.tensor(mnist_train_features.values, dtype=torch.float)\n",
    "x_test_tensor = torch.tensor(mnist_test_features.values, dtype=torch.float)\n",
    "\n",
    "Y_train_tensor = torch.tensor(mnist_train_target.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(mnist_test_target.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22193275-4ddc-44fe-9f25-e77cfbb0748d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]), torch.Size([60000]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the shape of these tensors.\n",
    "# Here are our training features and the corresponding training labels.\n",
    "# There are 60000 images, and each image has 784 pixels,\n",
    "# and there are 60000 corresponding labels as well.\n",
    "\n",
    "X_train_tensor.shape, Y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "90d936d4-03de-4f36-8dac-a730a3c2b0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 784]), torch.Size([10000]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When we feed in images to a deep fully-connected (FC) neural network,\n",
    "# we'll feed them in as one-dimensional vectors.\n",
    "# Deep FC neural networks do not accept images in the original\n",
    "# two-dimensional format with height and width.\n",
    "\n",
    "\n",
    "# Let's take a look at our test data, there are 10000 test images that we'll use to evaluate our model.\n",
    "x_test_tensor.shape, y_test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46ded7-3808-4628-b25e-775e7bc4ced4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
