{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2ad8806b-7aef-4255-ae57-a05aa22501ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "922f5ce7-8825-45c2-b900-18ff55b0467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the data\n",
    "data = pd.read_csv('homework_exampledata.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7e8c33ab-1987-4544-99cc-24c9a8833954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-up data from nan-values\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "328217d4-000d-43be-b60f-c9edea6a44cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49977, 6)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the shape of the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2e28a121-e107-43e6-9b4a-9c613e2547c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Pre-process the data\n",
    "# Convert time to a relative time (it's not strictly necessary but sometimes helps)\n",
    "data['time'] = (data['time'] - data['time'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c8188baa-08c6-4267-90fe-07178384512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split the dataset\n",
    "features = data[['time', 'brake-value', 'yaw-value', 'longitudinal-acceleration', 'lateral-acceleration']]\n",
    "labels = data['velocity-value']\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9c99be42-82d9-465f-9d1a-3ad2e8115bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the standardizer\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both the training and testing data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "230491a0-0353-4866-ac57-c897dc46651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-max normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit on the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both the training and test data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c1a0ee1d-3e19-43ae-957c-5c241a5fb5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39981, 5), (39981,))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of training set\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5bd6fe7f-b3ee-4a96-8a8b-a9a6982e4e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9996, 5), (9996,))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of a test set\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f108eb2-951e-4a99-bbc6-c2cd8197f952",
   "metadata": {},
   "source": [
    "# Appy Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "29c826da-688e-497a-8c6f-aebb496e19ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Train a regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "95f8e52d-4303-4e80-8edc-9c97865b103a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 322.73665978714416\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10957147-107e-4485-ba4e-b1b27b0b2101",
   "metadata": {},
   "source": [
    "# Apply Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "75b0a230-b2e5-48a6-a63f-1bbc515a4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f898651a-add6-42cf-818d-64812a533100",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=300, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7b86a826-548d-4b80-a4ae-f1fa1dc6f08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=300, random_state=42)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0da78555-9cd3-41e3-a162-083f25078407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error with Random Forest: 5.825557230152839\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error with Random Forest: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d2aa4-f7e3-412a-9589-d54242f8850f",
   "metadata": {},
   "source": [
    "# Apply Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6e48d881-c9d3-467b-ad2e-6abd3551a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c14982dc-3e39-4e28-8c41-c84a53942f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradient Boosting Regressor model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.1, max_depth=8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "926e9658-325a-414c-8966-26f7c0bc1164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=8, n_estimators=300, random_state=42)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0b7e6642-d6fe-4019-be5d-496b6f9d11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make predictions on the test data\n",
    "gb_predictions = gb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0fe2bcf0-9db2-48f3-b2b0-daa554d815e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean squared error of the predictions\n",
    "mse_gb = mean_squared_error(y_test, gb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9dd817d0-50e0-4e6c-9454-4fe0dd93b985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error with Gradient Boosting: 1.951895057907476\n"
     ]
    }
   ],
   "source": [
    "# Output the mean squared error\n",
    "print(f'Mean Squared Error with Gradient Boosting: {mse_gb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f6159-6b22-462e-a43e-8d7fa29e61a4",
   "metadata": {},
   "source": [
    "# Apply XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3a560570-8000-442f-883a-7829b77f5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "aa762524-dfc2-49fc-afd7-fdfd0741f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators=300, learning_rate=0.1, max_depth=8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a30b7dbf-eb7c-411e-b362-33492d17580a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=8, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=300, n_jobs=12,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=42,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6b5b58f6-0ab1-4dab-aa01-3dd5ff7cf0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make predictions on the test data\n",
    "xgb_predictions = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7ee298b5-151b-475c-af40-fa960edc5ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean squared error of the predictions\n",
    "mse_xgb = mean_squared_error(y_test, xgb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e39171ac-4608-4413-85d4-37f30691b92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error with XGBoost: 1.9872652737726209\n"
     ]
    }
   ],
   "source": [
    "# Output the mean squared error\n",
    "print(f'Mean Squared Error with XGBoost: {mse_xgb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe4a6c6-887a-405e-b302-0cccefa50fe7",
   "metadata": {},
   "source": [
    "# Apply LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "43b85375-165f-4693-88bc-9343c40bb07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "81db8e3e-a246-4809-bac7-d420d2d2e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=300, learning_rate=0.1, max_depth=8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e61548c2-f0ba-4aed-b3ee-f797a78651f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(max_depth=8, n_estimators=300, random_state=42)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "lgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a595624b-e14b-426c-ae2e-7646db850656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make predictions on the test data\n",
    "lgb_predictions = lgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "78be1827-2053-4454-a237-729002c51770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean squared error of the predictions\n",
    "mse_lgb = mean_squared_error(y_test, lgb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "831fe34d-e5af-4879-a4d5-a4733c7f033e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error with LightGBM: 12.820186582456426\n"
     ]
    }
   ],
   "source": [
    "# Output the mean squared error\n",
    "print(f'Mean Squared Error with LightGBM: {mse_lgb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc40e4-c734-49ec-919b-025af87ec1f1",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4e5439ba-54f6-4100-b239-861e44005201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f8bbe4b9-55cc-46b7-bf91-ed256e782570",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.values[:, 0:5] # features\n",
    "y = data.values[:, 5] # labels(velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a87017a4-7892-4fc0-8911-09ee74907d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "93a114e5-b74f-424f-8ac0-0a213b61f999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49977, 5), (49977,))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "661726aa-2e3f-46dc-b9aa-87300eea122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f1397295-384a-435a-b542-3e97c5e7c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "514f41a6-5790-4096-95fc-d3ec3a2d2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "57294dd7-2b4b-4c03-986f-898ddf431607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7075f23f-8cb4-4744-88f5-bc4df359172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 10\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6eae6543-6a1c-4027-ab2b-f438133433a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7b9bab14-735f-4e6f-9d6f-593c2e5ecfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 617.89697265625\n",
      "Epoch [20/10000], Loss: 594.3485107421875\n",
      "Epoch [30/10000], Loss: 563.5932006835938\n",
      "Epoch [40/10000], Loss: 524.334228515625\n",
      "Epoch [50/10000], Loss: 477.91119384765625\n",
      "Epoch [60/10000], Loss: 428.2763671875\n",
      "Epoch [70/10000], Loss: 381.4322509765625\n",
      "Epoch [80/10000], Loss: 344.00408935546875\n",
      "Epoch [90/10000], Loss: 319.67864990234375\n",
      "Epoch [100/10000], Loss: 307.0384216308594\n",
      "Epoch [110/10000], Loss: 301.1793212890625\n",
      "Epoch [120/10000], Loss: 297.4721984863281\n",
      "Epoch [130/10000], Loss: 294.13421630859375\n",
      "Epoch [140/10000], Loss: 291.0730285644531\n",
      "Epoch [150/10000], Loss: 288.4329528808594\n",
      "Epoch [160/10000], Loss: 286.1910400390625\n",
      "Epoch [170/10000], Loss: 284.2213134765625\n",
      "Epoch [180/10000], Loss: 282.494140625\n",
      "Epoch [190/10000], Loss: 281.0892028808594\n",
      "Epoch [200/10000], Loss: 279.87603759765625\n",
      "Epoch [210/10000], Loss: 278.7787780761719\n",
      "Epoch [220/10000], Loss: 277.78216552734375\n",
      "Epoch [230/10000], Loss: 276.90399169921875\n",
      "Epoch [240/10000], Loss: 275.9508361816406\n",
      "Epoch [250/10000], Loss: 274.8956604003906\n",
      "Epoch [260/10000], Loss: 273.7661437988281\n",
      "Epoch [270/10000], Loss: 272.49798583984375\n",
      "Epoch [280/10000], Loss: 271.0221252441406\n",
      "Epoch [290/10000], Loss: 269.36474609375\n",
      "Epoch [300/10000], Loss: 267.5281982421875\n",
      "Epoch [310/10000], Loss: 265.57989501953125\n",
      "Epoch [320/10000], Loss: 263.5605773925781\n",
      "Epoch [330/10000], Loss: 261.62677001953125\n",
      "Epoch [340/10000], Loss: 259.7490539550781\n",
      "Epoch [350/10000], Loss: 257.9253234863281\n",
      "Epoch [360/10000], Loss: 256.1777648925781\n",
      "Epoch [370/10000], Loss: 254.49496459960938\n",
      "Epoch [380/10000], Loss: 252.87547302246094\n",
      "Epoch [390/10000], Loss: 251.30487060546875\n",
      "Epoch [400/10000], Loss: 249.796630859375\n",
      "Epoch [410/10000], Loss: 248.46612548828125\n",
      "Epoch [420/10000], Loss: 247.1888427734375\n",
      "Epoch [430/10000], Loss: 245.9579315185547\n",
      "Epoch [440/10000], Loss: 244.75296020507812\n",
      "Epoch [450/10000], Loss: 243.58935546875\n",
      "Epoch [460/10000], Loss: 242.47267150878906\n",
      "Epoch [470/10000], Loss: 241.419677734375\n",
      "Epoch [480/10000], Loss: 240.42355346679688\n",
      "Epoch [490/10000], Loss: 239.47015380859375\n",
      "Epoch [500/10000], Loss: 238.54885864257812\n",
      "Epoch [510/10000], Loss: 237.64747619628906\n",
      "Epoch [520/10000], Loss: 236.7772979736328\n",
      "Epoch [530/10000], Loss: 235.93234252929688\n",
      "Epoch [540/10000], Loss: 235.11767578125\n",
      "Epoch [550/10000], Loss: 234.33131408691406\n",
      "Epoch [560/10000], Loss: 233.55108642578125\n",
      "Epoch [570/10000], Loss: 232.7661590576172\n",
      "Epoch [580/10000], Loss: 231.9709930419922\n",
      "Epoch [590/10000], Loss: 231.1543426513672\n",
      "Epoch [600/10000], Loss: 230.27598571777344\n",
      "Epoch [610/10000], Loss: 229.26385498046875\n",
      "Epoch [620/10000], Loss: 228.20982360839844\n",
      "Epoch [630/10000], Loss: 227.3546142578125\n",
      "Epoch [640/10000], Loss: 226.51300048828125\n",
      "Epoch [650/10000], Loss: 225.7069091796875\n",
      "Epoch [660/10000], Loss: 224.90032958984375\n",
      "Epoch [670/10000], Loss: 224.09152221679688\n",
      "Epoch [680/10000], Loss: 223.3030242919922\n",
      "Epoch [690/10000], Loss: 222.51219177246094\n",
      "Epoch [700/10000], Loss: 221.7720947265625\n",
      "Epoch [710/10000], Loss: 221.04742431640625\n",
      "Epoch [720/10000], Loss: 220.33518981933594\n",
      "Epoch [730/10000], Loss: 219.644775390625\n",
      "Epoch [740/10000], Loss: 218.96861267089844\n",
      "Epoch [750/10000], Loss: 218.29774475097656\n",
      "Epoch [760/10000], Loss: 217.6414337158203\n",
      "Epoch [770/10000], Loss: 216.9906463623047\n",
      "Epoch [780/10000], Loss: 216.3450927734375\n",
      "Epoch [790/10000], Loss: 215.68858337402344\n",
      "Epoch [800/10000], Loss: 215.0542449951172\n",
      "Epoch [810/10000], Loss: 214.46652221679688\n",
      "Epoch [820/10000], Loss: 213.88954162597656\n",
      "Epoch [830/10000], Loss: 213.3213348388672\n",
      "Epoch [840/10000], Loss: 212.76345825195312\n",
      "Epoch [850/10000], Loss: 212.21910095214844\n",
      "Epoch [860/10000], Loss: 211.68565368652344\n",
      "Epoch [870/10000], Loss: 211.1602020263672\n",
      "Epoch [880/10000], Loss: 210.64317321777344\n",
      "Epoch [890/10000], Loss: 210.13058471679688\n",
      "Epoch [900/10000], Loss: 209.62258911132812\n",
      "Epoch [910/10000], Loss: 209.11544799804688\n",
      "Epoch [920/10000], Loss: 208.616455078125\n",
      "Epoch [930/10000], Loss: 208.1227569580078\n",
      "Epoch [940/10000], Loss: 207.63104248046875\n",
      "Epoch [950/10000], Loss: 207.13999938964844\n",
      "Epoch [960/10000], Loss: 206.6528778076172\n",
      "Epoch [970/10000], Loss: 206.1676788330078\n",
      "Epoch [980/10000], Loss: 205.6720428466797\n",
      "Epoch [990/10000], Loss: 205.16665649414062\n",
      "Epoch [1000/10000], Loss: 204.6575927734375\n",
      "Epoch [1010/10000], Loss: 204.13975524902344\n",
      "Epoch [1020/10000], Loss: 203.6117401123047\n",
      "Epoch [1030/10000], Loss: 203.0828094482422\n",
      "Epoch [1040/10000], Loss: 202.53675842285156\n",
      "Epoch [1050/10000], Loss: 201.96319580078125\n",
      "Epoch [1060/10000], Loss: 201.34747314453125\n",
      "Epoch [1070/10000], Loss: 200.68243408203125\n",
      "Epoch [1080/10000], Loss: 200.13253784179688\n",
      "Epoch [1090/10000], Loss: 199.52432250976562\n",
      "Epoch [1100/10000], Loss: 198.81576538085938\n",
      "Epoch [1110/10000], Loss: 197.82806396484375\n",
      "Epoch [1120/10000], Loss: 195.99635314941406\n",
      "Epoch [1130/10000], Loss: 195.08056640625\n",
      "Epoch [1140/10000], Loss: 194.1815948486328\n",
      "Epoch [1150/10000], Loss: 193.32215881347656\n",
      "Epoch [1160/10000], Loss: 192.4044952392578\n",
      "Epoch [1170/10000], Loss: 191.35586547851562\n",
      "Epoch [1180/10000], Loss: 190.51409912109375\n",
      "Epoch [1190/10000], Loss: 189.9029083251953\n",
      "Epoch [1200/10000], Loss: 189.3673553466797\n",
      "Epoch [1210/10000], Loss: 188.85252380371094\n",
      "Epoch [1220/10000], Loss: 188.35189819335938\n",
      "Epoch [1230/10000], Loss: 187.84542846679688\n",
      "Epoch [1240/10000], Loss: 187.34390258789062\n",
      "Epoch [1250/10000], Loss: 186.84519958496094\n",
      "Epoch [1260/10000], Loss: 186.35342407226562\n",
      "Epoch [1270/10000], Loss: 185.8612518310547\n",
      "Epoch [1280/10000], Loss: 185.37484741210938\n",
      "Epoch [1290/10000], Loss: 184.9224853515625\n",
      "Epoch [1300/10000], Loss: 184.49269104003906\n",
      "Epoch [1310/10000], Loss: 184.0774383544922\n",
      "Epoch [1320/10000], Loss: 183.66688537597656\n",
      "Epoch [1330/10000], Loss: 183.2511444091797\n",
      "Epoch [1340/10000], Loss: 182.834228515625\n",
      "Epoch [1350/10000], Loss: 182.4214630126953\n",
      "Epoch [1360/10000], Loss: 182.01278686523438\n",
      "Epoch [1370/10000], Loss: 181.61302185058594\n",
      "Epoch [1380/10000], Loss: 181.2218780517578\n",
      "Epoch [1390/10000], Loss: 180.8316192626953\n",
      "Epoch [1400/10000], Loss: 180.45111083984375\n",
      "Epoch [1410/10000], Loss: 180.08175659179688\n",
      "Epoch [1420/10000], Loss: 179.71922302246094\n",
      "Epoch [1430/10000], Loss: 179.35888671875\n",
      "Epoch [1440/10000], Loss: 179.01345825195312\n",
      "Epoch [1450/10000], Loss: 178.6765899658203\n",
      "Epoch [1460/10000], Loss: 178.34783935546875\n",
      "Epoch [1470/10000], Loss: 178.0235595703125\n",
      "Epoch [1480/10000], Loss: 177.70599365234375\n",
      "Epoch [1490/10000], Loss: 177.39268493652344\n",
      "Epoch [1500/10000], Loss: 177.0881805419922\n",
      "Epoch [1510/10000], Loss: 176.79165649414062\n",
      "Epoch [1520/10000], Loss: 176.5024871826172\n",
      "Epoch [1530/10000], Loss: 176.22213745117188\n",
      "Epoch [1540/10000], Loss: 175.9472198486328\n",
      "Epoch [1550/10000], Loss: 175.67694091796875\n",
      "Epoch [1560/10000], Loss: 175.41107177734375\n",
      "Epoch [1570/10000], Loss: 175.14926147460938\n",
      "Epoch [1580/10000], Loss: 174.89175415039062\n",
      "Epoch [1590/10000], Loss: 174.6382598876953\n",
      "Epoch [1600/10000], Loss: 174.3885040283203\n",
      "Epoch [1610/10000], Loss: 174.1414337158203\n",
      "Epoch [1620/10000], Loss: 173.8936767578125\n",
      "Epoch [1630/10000], Loss: 173.6444549560547\n",
      "Epoch [1640/10000], Loss: 173.39295959472656\n",
      "Epoch [1650/10000], Loss: 173.14527893066406\n",
      "Epoch [1660/10000], Loss: 172.90057373046875\n",
      "Epoch [1670/10000], Loss: 172.6576690673828\n",
      "Epoch [1680/10000], Loss: 172.39598083496094\n",
      "Epoch [1690/10000], Loss: 172.14027404785156\n",
      "Epoch [1700/10000], Loss: 171.89598083496094\n",
      "Epoch [1710/10000], Loss: 171.6617889404297\n",
      "Epoch [1720/10000], Loss: 171.43798828125\n",
      "Epoch [1730/10000], Loss: 171.21519470214844\n",
      "Epoch [1740/10000], Loss: 170.98841857910156\n",
      "Epoch [1750/10000], Loss: 170.7630615234375\n",
      "Epoch [1760/10000], Loss: 170.53941345214844\n",
      "Epoch [1770/10000], Loss: 170.32354736328125\n",
      "Epoch [1780/10000], Loss: 170.10906982421875\n",
      "Epoch [1790/10000], Loss: 169.89633178710938\n",
      "Epoch [1800/10000], Loss: 169.6845245361328\n",
      "Epoch [1810/10000], Loss: 169.47535705566406\n",
      "Epoch [1820/10000], Loss: 169.26614379882812\n",
      "Epoch [1830/10000], Loss: 169.05885314941406\n",
      "Epoch [1840/10000], Loss: 168.85446166992188\n",
      "Epoch [1850/10000], Loss: 168.65365600585938\n",
      "Epoch [1860/10000], Loss: 168.45228576660156\n",
      "Epoch [1870/10000], Loss: 168.254150390625\n",
      "Epoch [1880/10000], Loss: 168.05747985839844\n",
      "Epoch [1890/10000], Loss: 167.8630828857422\n",
      "Epoch [1900/10000], Loss: 167.67015075683594\n",
      "Epoch [1910/10000], Loss: 167.47933959960938\n",
      "Epoch [1920/10000], Loss: 167.29238891601562\n",
      "Epoch [1930/10000], Loss: 167.1063232421875\n",
      "Epoch [1940/10000], Loss: 166.91934204101562\n",
      "Epoch [1950/10000], Loss: 166.73487854003906\n",
      "Epoch [1960/10000], Loss: 166.55340576171875\n",
      "Epoch [1970/10000], Loss: 166.3751220703125\n",
      "Epoch [1980/10000], Loss: 166.20089721679688\n",
      "Epoch [1990/10000], Loss: 166.0272216796875\n",
      "Epoch [2000/10000], Loss: 165.85682678222656\n",
      "Epoch [2010/10000], Loss: 165.6819305419922\n",
      "Epoch [2020/10000], Loss: 165.50668334960938\n",
      "Epoch [2030/10000], Loss: 165.33657836914062\n",
      "Epoch [2040/10000], Loss: 165.16751098632812\n",
      "Epoch [2050/10000], Loss: 164.9990692138672\n",
      "Epoch [2060/10000], Loss: 164.83102416992188\n",
      "Epoch [2070/10000], Loss: 164.6632843017578\n",
      "Epoch [2080/10000], Loss: 164.5011749267578\n",
      "Epoch [2090/10000], Loss: 164.34329223632812\n",
      "Epoch [2100/10000], Loss: 164.18751525878906\n",
      "Epoch [2110/10000], Loss: 164.03404235839844\n",
      "Epoch [2120/10000], Loss: 163.880615234375\n",
      "Epoch [2130/10000], Loss: 163.7275390625\n",
      "Epoch [2140/10000], Loss: 163.5759735107422\n",
      "Epoch [2150/10000], Loss: 163.4268035888672\n",
      "Epoch [2160/10000], Loss: 163.27536010742188\n",
      "Epoch [2170/10000], Loss: 163.12110900878906\n",
      "Epoch [2180/10000], Loss: 162.97528076171875\n",
      "Epoch [2190/10000], Loss: 162.835693359375\n",
      "Epoch [2200/10000], Loss: 162.7001953125\n",
      "Epoch [2210/10000], Loss: 162.5677947998047\n",
      "Epoch [2220/10000], Loss: 162.4320831298828\n",
      "Epoch [2230/10000], Loss: 162.30023193359375\n",
      "Epoch [2240/10000], Loss: 162.17369079589844\n",
      "Epoch [2250/10000], Loss: 162.0507354736328\n",
      "Epoch [2260/10000], Loss: 161.92581176757812\n",
      "Epoch [2270/10000], Loss: 161.8104248046875\n",
      "Epoch [2280/10000], Loss: 161.6934356689453\n",
      "Epoch [2290/10000], Loss: 161.58216857910156\n",
      "Epoch [2300/10000], Loss: 161.46633911132812\n",
      "Epoch [2310/10000], Loss: 161.35342407226562\n",
      "Epoch [2320/10000], Loss: 161.24586486816406\n",
      "Epoch [2330/10000], Loss: 161.1368408203125\n",
      "Epoch [2340/10000], Loss: 161.02008056640625\n",
      "Epoch [2350/10000], Loss: 160.91281127929688\n",
      "Epoch [2360/10000], Loss: 160.8074493408203\n",
      "Epoch [2370/10000], Loss: 160.70233154296875\n",
      "Epoch [2380/10000], Loss: 160.60130310058594\n",
      "Epoch [2390/10000], Loss: 160.50436401367188\n",
      "Epoch [2400/10000], Loss: 160.40924072265625\n",
      "Epoch [2410/10000], Loss: 160.31341552734375\n",
      "Epoch [2420/10000], Loss: 160.22653198242188\n",
      "Epoch [2430/10000], Loss: 160.13070678710938\n",
      "Epoch [2440/10000], Loss: 160.04592895507812\n",
      "Epoch [2450/10000], Loss: 159.9542999267578\n",
      "Epoch [2460/10000], Loss: 159.86831665039062\n",
      "Epoch [2470/10000], Loss: 159.78628540039062\n",
      "Epoch [2480/10000], Loss: 159.7036590576172\n",
      "Epoch [2490/10000], Loss: 159.62301635742188\n",
      "Epoch [2500/10000], Loss: 159.54067993164062\n",
      "Epoch [2510/10000], Loss: 159.46148681640625\n",
      "Epoch [2520/10000], Loss: 159.39205932617188\n",
      "Epoch [2530/10000], Loss: 159.3042755126953\n",
      "Epoch [2540/10000], Loss: 159.22775268554688\n",
      "Epoch [2550/10000], Loss: 159.149658203125\n",
      "Epoch [2560/10000], Loss: 159.0756378173828\n",
      "Epoch [2570/10000], Loss: 159.0\n",
      "Epoch [2580/10000], Loss: 158.93321228027344\n",
      "Epoch [2590/10000], Loss: 158.8574981689453\n",
      "Epoch [2600/10000], Loss: 158.79177856445312\n",
      "Epoch [2610/10000], Loss: 158.7232208251953\n",
      "Epoch [2620/10000], Loss: 158.6646270751953\n",
      "Epoch [2630/10000], Loss: 158.58908081054688\n",
      "Epoch [2640/10000], Loss: 158.5323486328125\n",
      "Epoch [2650/10000], Loss: 158.46702575683594\n",
      "Epoch [2660/10000], Loss: 158.40000915527344\n",
      "Epoch [2670/10000], Loss: 158.34121704101562\n",
      "Epoch [2680/10000], Loss: 158.27896118164062\n",
      "Epoch [2690/10000], Loss: 158.2247314453125\n",
      "Epoch [2700/10000], Loss: 158.17042541503906\n",
      "Epoch [2710/10000], Loss: 158.11370849609375\n",
      "Epoch [2720/10000], Loss: 158.0543975830078\n",
      "Epoch [2730/10000], Loss: 157.99891662597656\n",
      "Epoch [2740/10000], Loss: 157.94580078125\n",
      "Epoch [2750/10000], Loss: 157.89120483398438\n",
      "Epoch [2760/10000], Loss: 157.83905029296875\n",
      "Epoch [2770/10000], Loss: 157.78701782226562\n",
      "Epoch [2780/10000], Loss: 157.73382568359375\n",
      "Epoch [2790/10000], Loss: 157.69345092773438\n",
      "Epoch [2800/10000], Loss: 157.63211059570312\n",
      "Epoch [2810/10000], Loss: 157.5762939453125\n",
      "Epoch [2820/10000], Loss: 157.51943969726562\n",
      "Epoch [2830/10000], Loss: 157.47091674804688\n",
      "Epoch [2840/10000], Loss: 157.42262268066406\n",
      "Epoch [2850/10000], Loss: 157.37335205078125\n",
      "Epoch [2860/10000], Loss: 157.32333374023438\n",
      "Epoch [2870/10000], Loss: 157.27427673339844\n",
      "Epoch [2880/10000], Loss: 157.228759765625\n",
      "Epoch [2890/10000], Loss: 157.1869354248047\n",
      "Epoch [2900/10000], Loss: 157.13291931152344\n",
      "Epoch [2910/10000], Loss: 157.08937072753906\n",
      "Epoch [2920/10000], Loss: 157.04322814941406\n",
      "Epoch [2930/10000], Loss: 156.99920654296875\n",
      "Epoch [2940/10000], Loss: 156.95404052734375\n",
      "Epoch [2950/10000], Loss: 156.9182586669922\n",
      "Epoch [2960/10000], Loss: 156.86688232421875\n",
      "Epoch [2970/10000], Loss: 156.8302459716797\n",
      "Epoch [2980/10000], Loss: 156.78688049316406\n",
      "Epoch [2990/10000], Loss: 156.7476348876953\n",
      "Epoch [3000/10000], Loss: 156.70486450195312\n",
      "Epoch [3010/10000], Loss: 156.6632080078125\n",
      "Epoch [3020/10000], Loss: 156.62582397460938\n",
      "Epoch [3030/10000], Loss: 156.58615112304688\n",
      "Epoch [3040/10000], Loss: 156.54489135742188\n",
      "Epoch [3050/10000], Loss: 156.50502014160156\n",
      "Epoch [3060/10000], Loss: 156.47459411621094\n",
      "Epoch [3070/10000], Loss: 156.42967224121094\n",
      "Epoch [3080/10000], Loss: 156.38766479492188\n",
      "Epoch [3090/10000], Loss: 156.34762573242188\n",
      "Epoch [3100/10000], Loss: 156.2982635498047\n",
      "Epoch [3110/10000], Loss: 156.25392150878906\n",
      "Epoch [3120/10000], Loss: 156.21852111816406\n",
      "Epoch [3130/10000], Loss: 156.1807403564453\n",
      "Epoch [3140/10000], Loss: 156.14117431640625\n",
      "Epoch [3150/10000], Loss: 156.1090545654297\n",
      "Epoch [3160/10000], Loss: 156.0832061767578\n",
      "Epoch [3170/10000], Loss: 156.0370330810547\n",
      "Epoch [3180/10000], Loss: 156.0041046142578\n",
      "Epoch [3190/10000], Loss: 155.96917724609375\n",
      "Epoch [3200/10000], Loss: 155.93734741210938\n",
      "Epoch [3210/10000], Loss: 155.90338134765625\n",
      "Epoch [3220/10000], Loss: 155.87094116210938\n",
      "Epoch [3230/10000], Loss: 155.838623046875\n",
      "Epoch [3240/10000], Loss: 155.80601501464844\n",
      "Epoch [3250/10000], Loss: 155.77520751953125\n",
      "Epoch [3260/10000], Loss: 155.74525451660156\n",
      "Epoch [3270/10000], Loss: 155.72898864746094\n",
      "Epoch [3280/10000], Loss: 155.68194580078125\n",
      "Epoch [3290/10000], Loss: 155.65121459960938\n",
      "Epoch [3300/10000], Loss: 155.62037658691406\n",
      "Epoch [3310/10000], Loss: 155.58740234375\n",
      "Epoch [3320/10000], Loss: 155.55638122558594\n",
      "Epoch [3330/10000], Loss: 155.52659606933594\n",
      "Epoch [3340/10000], Loss: 155.49691772460938\n",
      "Epoch [3350/10000], Loss: 155.47024536132812\n",
      "Epoch [3360/10000], Loss: 155.43881225585938\n",
      "Epoch [3370/10000], Loss: 155.42391967773438\n",
      "Epoch [3380/10000], Loss: 155.3848876953125\n",
      "Epoch [3390/10000], Loss: 155.3560028076172\n",
      "Epoch [3400/10000], Loss: 155.32901000976562\n",
      "Epoch [3410/10000], Loss: 155.30104064941406\n",
      "Epoch [3420/10000], Loss: 155.275634765625\n",
      "Epoch [3430/10000], Loss: 155.24765014648438\n",
      "Epoch [3440/10000], Loss: 155.22555541992188\n",
      "Epoch [3450/10000], Loss: 155.2122344970703\n",
      "Epoch [3460/10000], Loss: 155.17665100097656\n",
      "Epoch [3470/10000], Loss: 155.1487274169922\n",
      "Epoch [3480/10000], Loss: 155.12432861328125\n",
      "Epoch [3490/10000], Loss: 155.10093688964844\n",
      "Epoch [3500/10000], Loss: 155.07586669921875\n",
      "Epoch [3510/10000], Loss: 155.05419921875\n",
      "Epoch [3520/10000], Loss: 155.0299072265625\n",
      "Epoch [3530/10000], Loss: 155.02061462402344\n",
      "Epoch [3540/10000], Loss: 154.98423767089844\n",
      "Epoch [3550/10000], Loss: 154.96080017089844\n",
      "Epoch [3560/10000], Loss: 154.94076538085938\n",
      "Epoch [3570/10000], Loss: 154.9175567626953\n",
      "Epoch [3580/10000], Loss: 154.90162658691406\n",
      "Epoch [3590/10000], Loss: 154.8755645751953\n",
      "Epoch [3600/10000], Loss: 154.8547821044922\n",
      "Epoch [3610/10000], Loss: 154.8330078125\n",
      "Epoch [3620/10000], Loss: 154.81076049804688\n",
      "Epoch [3630/10000], Loss: 154.7895050048828\n",
      "Epoch [3640/10000], Loss: 154.76878356933594\n",
      "Epoch [3650/10000], Loss: 154.7538604736328\n",
      "Epoch [3660/10000], Loss: 154.74684143066406\n",
      "Epoch [3670/10000], Loss: 154.71583557128906\n",
      "Epoch [3680/10000], Loss: 154.6928253173828\n",
      "Epoch [3690/10000], Loss: 154.67111206054688\n",
      "Epoch [3700/10000], Loss: 154.65142822265625\n",
      "Epoch [3710/10000], Loss: 154.63259887695312\n",
      "Epoch [3720/10000], Loss: 154.61216735839844\n",
      "Epoch [3730/10000], Loss: 154.59378051757812\n",
      "Epoch [3740/10000], Loss: 154.57151794433594\n",
      "Epoch [3750/10000], Loss: 154.55526733398438\n",
      "Epoch [3760/10000], Loss: 154.55123901367188\n",
      "Epoch [3770/10000], Loss: 154.52162170410156\n",
      "Epoch [3780/10000], Loss: 154.50100708007812\n",
      "Epoch [3790/10000], Loss: 154.479736328125\n",
      "Epoch [3800/10000], Loss: 154.46102905273438\n",
      "Epoch [3810/10000], Loss: 154.4425811767578\n",
      "Epoch [3820/10000], Loss: 154.42489624023438\n",
      "Epoch [3830/10000], Loss: 154.40692138671875\n",
      "Epoch [3840/10000], Loss: 154.38963317871094\n",
      "Epoch [3850/10000], Loss: 154.3872833251953\n",
      "Epoch [3860/10000], Loss: 154.35464477539062\n",
      "Epoch [3870/10000], Loss: 154.3370361328125\n",
      "Epoch [3880/10000], Loss: 154.32115173339844\n",
      "Epoch [3890/10000], Loss: 154.3043975830078\n",
      "Epoch [3900/10000], Loss: 154.28729248046875\n",
      "Epoch [3910/10000], Loss: 154.27169799804688\n",
      "Epoch [3920/10000], Loss: 154.25469970703125\n",
      "Epoch [3930/10000], Loss: 154.23646545410156\n",
      "Epoch [3940/10000], Loss: 154.23843383789062\n",
      "Epoch [3950/10000], Loss: 154.20997619628906\n",
      "Epoch [3960/10000], Loss: 154.19564819335938\n",
      "Epoch [3970/10000], Loss: 154.1728515625\n",
      "Epoch [3980/10000], Loss: 154.1580047607422\n",
      "Epoch [3990/10000], Loss: 154.1384735107422\n",
      "Epoch [4000/10000], Loss: 154.1099090576172\n",
      "Epoch [4010/10000], Loss: 154.03228759765625\n",
      "Epoch [4020/10000], Loss: 153.94049072265625\n",
      "Epoch [4030/10000], Loss: 153.84573364257812\n",
      "Epoch [4040/10000], Loss: 153.7576141357422\n",
      "Epoch [4050/10000], Loss: 153.7024383544922\n",
      "Epoch [4060/10000], Loss: 153.6649932861328\n",
      "Epoch [4070/10000], Loss: 153.639404296875\n",
      "Epoch [4080/10000], Loss: 153.63319396972656\n",
      "Epoch [4090/10000], Loss: 153.64991760253906\n",
      "Epoch [4100/10000], Loss: 153.59576416015625\n",
      "Epoch [4110/10000], Loss: 153.5748291015625\n",
      "Epoch [4120/10000], Loss: 153.57064819335938\n",
      "Epoch [4130/10000], Loss: 153.56117248535156\n",
      "Epoch [4140/10000], Loss: 153.5431671142578\n",
      "Epoch [4150/10000], Loss: 153.52020263671875\n",
      "Epoch [4160/10000], Loss: 153.49913024902344\n",
      "Epoch [4170/10000], Loss: 153.48193359375\n",
      "Epoch [4180/10000], Loss: 153.4729766845703\n",
      "Epoch [4190/10000], Loss: 153.4636993408203\n",
      "Epoch [4200/10000], Loss: 153.45074462890625\n",
      "Epoch [4210/10000], Loss: 153.43185424804688\n",
      "Epoch [4220/10000], Loss: 153.41574096679688\n",
      "Epoch [4230/10000], Loss: 153.40011596679688\n",
      "Epoch [4240/10000], Loss: 153.39295959472656\n",
      "Epoch [4250/10000], Loss: 153.3636932373047\n",
      "Epoch [4260/10000], Loss: 153.35800170898438\n",
      "Epoch [4270/10000], Loss: 153.33816528320312\n",
      "Epoch [4280/10000], Loss: 153.32638549804688\n",
      "Epoch [4290/10000], Loss: 153.31411743164062\n",
      "Epoch [4300/10000], Loss: 153.2962188720703\n",
      "Epoch [4310/10000], Loss: 153.28675842285156\n",
      "Epoch [4320/10000], Loss: 153.28103637695312\n",
      "Epoch [4330/10000], Loss: 153.27330017089844\n",
      "Epoch [4340/10000], Loss: 153.24456787109375\n",
      "Epoch [4350/10000], Loss: 153.23265075683594\n",
      "Epoch [4360/10000], Loss: 153.219482421875\n",
      "Epoch [4370/10000], Loss: 153.20826721191406\n",
      "Epoch [4380/10000], Loss: 153.1973114013672\n",
      "Epoch [4390/10000], Loss: 153.1855926513672\n",
      "Epoch [4400/10000], Loss: 153.17236328125\n",
      "Epoch [4410/10000], Loss: 153.15895080566406\n",
      "Epoch [4420/10000], Loss: 153.1466522216797\n",
      "Epoch [4430/10000], Loss: 153.13943481445312\n",
      "Epoch [4440/10000], Loss: 153.12599182128906\n",
      "Epoch [4450/10000], Loss: 153.10968017578125\n",
      "Epoch [4460/10000], Loss: 153.09971618652344\n",
      "Epoch [4470/10000], Loss: 153.08531188964844\n",
      "Epoch [4480/10000], Loss: 153.0732421875\n",
      "Epoch [4490/10000], Loss: 153.0626220703125\n",
      "Epoch [4500/10000], Loss: 153.06678771972656\n",
      "Epoch [4510/10000], Loss: 153.03952026367188\n",
      "Epoch [4520/10000], Loss: 153.0282440185547\n",
      "Epoch [4530/10000], Loss: 153.01695251464844\n",
      "Epoch [4540/10000], Loss: 153.00531005859375\n",
      "Epoch [4550/10000], Loss: 152.99241638183594\n",
      "Epoch [4560/10000], Loss: 152.9788360595703\n",
      "Epoch [4570/10000], Loss: 152.96578979492188\n",
      "Epoch [4580/10000], Loss: 152.95538330078125\n",
      "Epoch [4590/10000], Loss: 152.94764709472656\n",
      "Epoch [4600/10000], Loss: 152.9048309326172\n",
      "Epoch [4610/10000], Loss: 152.8797607421875\n",
      "Epoch [4620/10000], Loss: 152.86753845214844\n",
      "Epoch [4630/10000], Loss: 152.8588104248047\n",
      "Epoch [4640/10000], Loss: 152.8408203125\n",
      "Epoch [4650/10000], Loss: 152.8284149169922\n",
      "Epoch [4660/10000], Loss: 152.82167053222656\n",
      "Epoch [4670/10000], Loss: 152.80690002441406\n",
      "Epoch [4680/10000], Loss: 152.8101348876953\n",
      "Epoch [4690/10000], Loss: 152.7914581298828\n",
      "Epoch [4700/10000], Loss: 152.78140258789062\n",
      "Epoch [4710/10000], Loss: 152.76608276367188\n",
      "Epoch [4720/10000], Loss: 152.7548065185547\n",
      "Epoch [4730/10000], Loss: 152.75241088867188\n",
      "Epoch [4740/10000], Loss: 152.73487854003906\n",
      "Epoch [4750/10000], Loss: 152.72433471679688\n",
      "Epoch [4760/10000], Loss: 152.72039794921875\n",
      "Epoch [4770/10000], Loss: 152.71310424804688\n",
      "Epoch [4780/10000], Loss: 152.70452880859375\n",
      "Epoch [4790/10000], Loss: 152.68508911132812\n",
      "Epoch [4800/10000], Loss: 152.6775360107422\n",
      "Epoch [4810/10000], Loss: 152.70831298828125\n",
      "Epoch [4820/10000], Loss: 152.66766357421875\n",
      "Epoch [4830/10000], Loss: 152.64308166503906\n",
      "Epoch [4840/10000], Loss: 152.63217163085938\n",
      "Epoch [4850/10000], Loss: 152.62257385253906\n",
      "Epoch [4860/10000], Loss: 152.61239624023438\n",
      "Epoch [4870/10000], Loss: 152.60203552246094\n",
      "Epoch [4880/10000], Loss: 152.5919647216797\n",
      "Epoch [4890/10000], Loss: 152.58189392089844\n",
      "Epoch [4900/10000], Loss: 152.57188415527344\n",
      "Epoch [4910/10000], Loss: 152.56407165527344\n",
      "Epoch [4920/10000], Loss: 152.5587615966797\n",
      "Epoch [4930/10000], Loss: 152.54246520996094\n",
      "Epoch [4940/10000], Loss: 152.5335693359375\n",
      "Epoch [4950/10000], Loss: 152.53562927246094\n",
      "Epoch [4960/10000], Loss: 152.5127716064453\n",
      "Epoch [4970/10000], Loss: 152.51039123535156\n",
      "Epoch [4980/10000], Loss: 152.49339294433594\n",
      "Epoch [4990/10000], Loss: 152.48590087890625\n",
      "Epoch [5000/10000], Loss: 152.47720336914062\n",
      "Epoch [5010/10000], Loss: 152.46737670898438\n",
      "Epoch [5020/10000], Loss: 152.4584503173828\n",
      "Epoch [5030/10000], Loss: 152.45510864257812\n",
      "Epoch [5040/10000], Loss: 152.43692016601562\n",
      "Epoch [5050/10000], Loss: 152.4298095703125\n",
      "Epoch [5060/10000], Loss: 152.41969299316406\n",
      "Epoch [5070/10000], Loss: 152.40621948242188\n",
      "Epoch [5080/10000], Loss: 152.39044189453125\n",
      "Epoch [5090/10000], Loss: 152.382080078125\n",
      "Epoch [5100/10000], Loss: 152.37130737304688\n",
      "Epoch [5110/10000], Loss: 152.39561462402344\n",
      "Epoch [5120/10000], Loss: 152.3595733642578\n",
      "Epoch [5130/10000], Loss: 152.35861206054688\n",
      "Epoch [5140/10000], Loss: 152.34353637695312\n",
      "Epoch [5150/10000], Loss: 152.3242645263672\n",
      "Epoch [5160/10000], Loss: 152.3150177001953\n",
      "Epoch [5170/10000], Loss: 152.3042449951172\n",
      "Epoch [5180/10000], Loss: 152.30319213867188\n",
      "Epoch [5190/10000], Loss: 152.28805541992188\n",
      "Epoch [5200/10000], Loss: 152.27879333496094\n",
      "Epoch [5210/10000], Loss: 152.2698516845703\n",
      "Epoch [5220/10000], Loss: 152.2630157470703\n",
      "Epoch [5230/10000], Loss: 152.26702880859375\n",
      "Epoch [5240/10000], Loss: 152.2442169189453\n",
      "Epoch [5250/10000], Loss: 152.2447967529297\n",
      "Epoch [5260/10000], Loss: 152.22801208496094\n",
      "Epoch [5270/10000], Loss: 152.21353149414062\n",
      "Epoch [5280/10000], Loss: 152.20347595214844\n",
      "Epoch [5290/10000], Loss: 152.19503784179688\n",
      "Epoch [5300/10000], Loss: 152.19894409179688\n",
      "Epoch [5310/10000], Loss: 152.1818084716797\n",
      "Epoch [5320/10000], Loss: 152.17153930664062\n",
      "Epoch [5330/10000], Loss: 152.1630859375\n",
      "Epoch [5340/10000], Loss: 152.15420532226562\n",
      "Epoch [5350/10000], Loss: 152.14442443847656\n",
      "Epoch [5360/10000], Loss: 152.13766479492188\n",
      "Epoch [5370/10000], Loss: 152.12796020507812\n",
      "Epoch [5380/10000], Loss: 152.12210083007812\n",
      "Epoch [5390/10000], Loss: 152.1195831298828\n",
      "Epoch [5400/10000], Loss: 152.10581970214844\n",
      "Epoch [5410/10000], Loss: 152.09622192382812\n",
      "Epoch [5420/10000], Loss: 152.09535217285156\n",
      "Epoch [5430/10000], Loss: 152.08531188964844\n",
      "Epoch [5440/10000], Loss: 152.07516479492188\n",
      "Epoch [5450/10000], Loss: 152.06768798828125\n",
      "Epoch [5460/10000], Loss: 152.05763244628906\n",
      "Epoch [5470/10000], Loss: 152.05203247070312\n",
      "Epoch [5480/10000], Loss: 152.06036376953125\n",
      "Epoch [5490/10000], Loss: 152.03480529785156\n",
      "Epoch [5500/10000], Loss: 152.0304718017578\n",
      "Epoch [5510/10000], Loss: 152.020263671875\n",
      "Epoch [5520/10000], Loss: 152.01710510253906\n",
      "Epoch [5530/10000], Loss: 152.00877380371094\n",
      "Epoch [5540/10000], Loss: 151.9993438720703\n",
      "Epoch [5550/10000], Loss: 151.99229431152344\n",
      "Epoch [5560/10000], Loss: 151.98983764648438\n",
      "Epoch [5570/10000], Loss: 151.98947143554688\n",
      "Epoch [5580/10000], Loss: 151.9702911376953\n",
      "Epoch [5590/10000], Loss: 151.96343994140625\n",
      "Epoch [5600/10000], Loss: 151.954833984375\n",
      "Epoch [5610/10000], Loss: 151.9488067626953\n",
      "Epoch [5620/10000], Loss: 151.95516967773438\n",
      "Epoch [5630/10000], Loss: 151.93746948242188\n",
      "Epoch [5640/10000], Loss: 151.92982482910156\n",
      "Epoch [5650/10000], Loss: 151.92100524902344\n",
      "Epoch [5660/10000], Loss: 151.91395568847656\n",
      "Epoch [5670/10000], Loss: 151.91378784179688\n",
      "Epoch [5680/10000], Loss: 151.90548706054688\n",
      "Epoch [5690/10000], Loss: 151.89479064941406\n",
      "Epoch [5700/10000], Loss: 151.89376831054688\n",
      "Epoch [5710/10000], Loss: 151.88497924804688\n",
      "Epoch [5720/10000], Loss: 151.87823486328125\n",
      "Epoch [5730/10000], Loss: 151.86952209472656\n",
      "Epoch [5740/10000], Loss: 151.90953063964844\n",
      "Epoch [5750/10000], Loss: 151.87379455566406\n",
      "Epoch [5760/10000], Loss: 151.85377502441406\n",
      "Epoch [5770/10000], Loss: 151.8444061279297\n",
      "Epoch [5780/10000], Loss: 151.83631896972656\n",
      "Epoch [5790/10000], Loss: 151.83103942871094\n",
      "Epoch [5800/10000], Loss: 151.82423400878906\n",
      "Epoch [5810/10000], Loss: 151.81521606445312\n",
      "Epoch [5820/10000], Loss: 151.81146240234375\n",
      "Epoch [5830/10000], Loss: 151.80482482910156\n",
      "Epoch [5840/10000], Loss: 151.79641723632812\n",
      "Epoch [5850/10000], Loss: 151.79025268554688\n",
      "Epoch [5860/10000], Loss: 151.7843780517578\n",
      "Epoch [5870/10000], Loss: 151.81248474121094\n",
      "Epoch [5880/10000], Loss: 151.7823944091797\n",
      "Epoch [5890/10000], Loss: 151.777099609375\n",
      "Epoch [5900/10000], Loss: 151.76165771484375\n",
      "Epoch [5910/10000], Loss: 151.75465393066406\n",
      "Epoch [5920/10000], Loss: 151.74935913085938\n",
      "Epoch [5930/10000], Loss: 151.74258422851562\n",
      "Epoch [5940/10000], Loss: 151.7367706298828\n",
      "Epoch [5950/10000], Loss: 151.73069763183594\n",
      "Epoch [5960/10000], Loss: 151.72486877441406\n",
      "Epoch [5970/10000], Loss: 151.7220001220703\n",
      "Epoch [5980/10000], Loss: 151.71971130371094\n",
      "Epoch [5990/10000], Loss: 151.7077178955078\n",
      "Epoch [6000/10000], Loss: 151.70498657226562\n",
      "Epoch [6010/10000], Loss: 151.69815063476562\n",
      "Epoch [6020/10000], Loss: 151.690673828125\n",
      "Epoch [6030/10000], Loss: 151.69422912597656\n",
      "Epoch [6040/10000], Loss: 151.68312072753906\n",
      "Epoch [6050/10000], Loss: 151.67333984375\n",
      "Epoch [6060/10000], Loss: 151.6698455810547\n",
      "Epoch [6070/10000], Loss: 151.66287231445312\n",
      "Epoch [6080/10000], Loss: 151.655029296875\n",
      "Epoch [6090/10000], Loss: 151.6543731689453\n",
      "Epoch [6100/10000], Loss: 151.65426635742188\n",
      "Epoch [6110/10000], Loss: 151.63900756835938\n",
      "Epoch [6120/10000], Loss: 151.6331024169922\n",
      "Epoch [6130/10000], Loss: 151.62619018554688\n",
      "Epoch [6140/10000], Loss: 151.6204376220703\n",
      "Epoch [6150/10000], Loss: 151.61935424804688\n",
      "Epoch [6160/10000], Loss: 151.63609313964844\n",
      "Epoch [6170/10000], Loss: 151.61048889160156\n",
      "Epoch [6180/10000], Loss: 151.59971618652344\n",
      "Epoch [6190/10000], Loss: 151.5943603515625\n",
      "Epoch [6200/10000], Loss: 151.58721923828125\n",
      "Epoch [6210/10000], Loss: 151.5848846435547\n",
      "Epoch [6220/10000], Loss: 151.581298828125\n",
      "Epoch [6230/10000], Loss: 151.57101440429688\n",
      "Epoch [6240/10000], Loss: 151.56605529785156\n",
      "Epoch [6250/10000], Loss: 151.5741424560547\n",
      "Epoch [6260/10000], Loss: 151.5558319091797\n",
      "Epoch [6270/10000], Loss: 151.5515899658203\n",
      "Epoch [6280/10000], Loss: 151.54010009765625\n",
      "Epoch [6290/10000], Loss: 151.5300750732422\n",
      "Epoch [6300/10000], Loss: 151.52880859375\n",
      "Epoch [6310/10000], Loss: 151.52259826660156\n",
      "Epoch [6320/10000], Loss: 151.5149383544922\n",
      "Epoch [6330/10000], Loss: 151.5127716064453\n",
      "Epoch [6340/10000], Loss: 151.5106964111328\n",
      "Epoch [6350/10000], Loss: 151.50779724121094\n",
      "Epoch [6360/10000], Loss: 151.49386596679688\n",
      "Epoch [6370/10000], Loss: 151.48773193359375\n",
      "Epoch [6380/10000], Loss: 151.48068237304688\n",
      "Epoch [6390/10000], Loss: 151.4772491455078\n",
      "Epoch [6400/10000], Loss: 151.4788818359375\n",
      "Epoch [6410/10000], Loss: 151.4955291748047\n",
      "Epoch [6420/10000], Loss: 151.47486877441406\n",
      "Epoch [6430/10000], Loss: 151.46481323242188\n",
      "Epoch [6440/10000], Loss: 151.45603942871094\n",
      "Epoch [6450/10000], Loss: 151.44692993164062\n",
      "Epoch [6460/10000], Loss: 151.4394073486328\n",
      "Epoch [6470/10000], Loss: 151.4332733154297\n",
      "Epoch [6480/10000], Loss: 151.4298095703125\n",
      "Epoch [6490/10000], Loss: 151.4259033203125\n",
      "Epoch [6500/10000], Loss: 151.41845703125\n",
      "Epoch [6510/10000], Loss: 151.4161376953125\n",
      "Epoch [6520/10000], Loss: 151.4208984375\n",
      "Epoch [6530/10000], Loss: 151.41650390625\n",
      "Epoch [6540/10000], Loss: 151.40806579589844\n",
      "Epoch [6550/10000], Loss: 151.39862060546875\n",
      "Epoch [6560/10000], Loss: 151.39263916015625\n",
      "Epoch [6570/10000], Loss: 151.38755798339844\n",
      "Epoch [6580/10000], Loss: 151.38784790039062\n",
      "Epoch [6590/10000], Loss: 151.38067626953125\n",
      "Epoch [6600/10000], Loss: 151.37802124023438\n",
      "Epoch [6610/10000], Loss: 151.37722778320312\n",
      "Epoch [6620/10000], Loss: 151.36474609375\n",
      "Epoch [6630/10000], Loss: 151.3601837158203\n",
      "Epoch [6640/10000], Loss: 151.35592651367188\n",
      "Epoch [6650/10000], Loss: 151.355712890625\n",
      "Epoch [6660/10000], Loss: 151.3683319091797\n",
      "Epoch [6670/10000], Loss: 151.3431854248047\n",
      "Epoch [6680/10000], Loss: 151.34080505371094\n",
      "Epoch [6690/10000], Loss: 151.33575439453125\n",
      "Epoch [6700/10000], Loss: 151.3331756591797\n",
      "Epoch [6710/10000], Loss: 151.32711791992188\n",
      "Epoch [6720/10000], Loss: 151.3314208984375\n",
      "Epoch [6730/10000], Loss: 151.3287811279297\n",
      "Epoch [6740/10000], Loss: 151.32403564453125\n",
      "Epoch [6750/10000], Loss: 151.3133087158203\n",
      "Epoch [6760/10000], Loss: 151.31289672851562\n",
      "Epoch [6770/10000], Loss: 151.30776977539062\n",
      "Epoch [6780/10000], Loss: 151.30592346191406\n",
      "Epoch [6790/10000], Loss: 151.3033447265625\n",
      "Epoch [6800/10000], Loss: 151.3180694580078\n",
      "Epoch [6810/10000], Loss: 151.28836059570312\n",
      "Epoch [6820/10000], Loss: 151.28240966796875\n",
      "Epoch [6830/10000], Loss: 151.2843780517578\n",
      "Epoch [6840/10000], Loss: 151.2799072265625\n",
      "Epoch [6850/10000], Loss: 151.27003479003906\n",
      "Epoch [6860/10000], Loss: 151.27149963378906\n",
      "Epoch [6870/10000], Loss: 151.2682647705078\n",
      "Epoch [6880/10000], Loss: 151.2705535888672\n",
      "Epoch [6890/10000], Loss: 151.25439453125\n",
      "Epoch [6900/10000], Loss: 151.25526428222656\n",
      "Epoch [6910/10000], Loss: 151.2879180908203\n",
      "Epoch [6920/10000], Loss: 151.2552947998047\n",
      "Epoch [6930/10000], Loss: 151.243408203125\n",
      "Epoch [6940/10000], Loss: 151.23464965820312\n",
      "Epoch [6950/10000], Loss: 151.23036193847656\n",
      "Epoch [6960/10000], Loss: 151.2268829345703\n",
      "Epoch [6970/10000], Loss: 151.22158813476562\n",
      "Epoch [6980/10000], Loss: 151.2189483642578\n",
      "Epoch [6990/10000], Loss: 151.2163543701172\n",
      "Epoch [7000/10000], Loss: 151.21072387695312\n",
      "Epoch [7010/10000], Loss: 151.20814514160156\n",
      "Epoch [7020/10000], Loss: 151.2291259765625\n",
      "Epoch [7030/10000], Loss: 151.20167541503906\n",
      "Epoch [7040/10000], Loss: 151.19482421875\n",
      "Epoch [7050/10000], Loss: 151.192138671875\n",
      "Epoch [7060/10000], Loss: 151.18893432617188\n",
      "Epoch [7070/10000], Loss: 151.18418884277344\n",
      "Epoch [7080/10000], Loss: 151.1817169189453\n",
      "Epoch [7090/10000], Loss: 151.1768341064453\n",
      "Epoch [7100/10000], Loss: 151.17538452148438\n",
      "Epoch [7110/10000], Loss: 151.18148803710938\n",
      "Epoch [7120/10000], Loss: 151.17237854003906\n",
      "Epoch [7130/10000], Loss: 151.16583251953125\n",
      "Epoch [7140/10000], Loss: 151.1585693359375\n",
      "Epoch [7150/10000], Loss: 151.156005859375\n",
      "Epoch [7160/10000], Loss: 151.1533966064453\n",
      "Epoch [7170/10000], Loss: 151.14939880371094\n",
      "Epoch [7180/10000], Loss: 151.14678955078125\n",
      "Epoch [7190/10000], Loss: 151.1451416015625\n",
      "Epoch [7200/10000], Loss: 151.15167236328125\n",
      "Epoch [7210/10000], Loss: 151.1344757080078\n",
      "Epoch [7220/10000], Loss: 151.1317138671875\n",
      "Epoch [7230/10000], Loss: 151.13101196289062\n",
      "Epoch [7240/10000], Loss: 151.1282196044922\n",
      "Epoch [7250/10000], Loss: 151.13551330566406\n",
      "Epoch [7260/10000], Loss: 151.12220764160156\n",
      "Epoch [7270/10000], Loss: 151.12139892578125\n",
      "Epoch [7280/10000], Loss: 151.1117706298828\n",
      "Epoch [7290/10000], Loss: 151.1079864501953\n",
      "Epoch [7300/10000], Loss: 151.126220703125\n",
      "Epoch [7310/10000], Loss: 151.11880493164062\n",
      "Epoch [7320/10000], Loss: 151.10069274902344\n",
      "Epoch [7330/10000], Loss: 151.09765625\n",
      "Epoch [7340/10000], Loss: 151.093017578125\n",
      "Epoch [7350/10000], Loss: 151.09039306640625\n",
      "Epoch [7360/10000], Loss: 151.0951385498047\n",
      "Epoch [7370/10000], Loss: 151.080810546875\n",
      "Epoch [7380/10000], Loss: 151.08079528808594\n",
      "Epoch [7390/10000], Loss: 151.0795440673828\n",
      "Epoch [7400/10000], Loss: 151.07528686523438\n",
      "Epoch [7410/10000], Loss: 151.072265625\n",
      "Epoch [7420/10000], Loss: 151.06459045410156\n",
      "Epoch [7430/10000], Loss: 151.07167053222656\n",
      "Epoch [7440/10000], Loss: 151.06895446777344\n",
      "Epoch [7450/10000], Loss: 151.05084228515625\n",
      "Epoch [7460/10000], Loss: 151.0491485595703\n",
      "Epoch [7470/10000], Loss: 151.04542541503906\n",
      "Epoch [7480/10000], Loss: 151.04151916503906\n",
      "Epoch [7490/10000], Loss: 151.03802490234375\n",
      "Epoch [7500/10000], Loss: 151.0343017578125\n",
      "Epoch [7510/10000], Loss: 151.03076171875\n",
      "Epoch [7520/10000], Loss: 151.028564453125\n",
      "Epoch [7530/10000], Loss: 151.02516174316406\n",
      "Epoch [7540/10000], Loss: 151.0236358642578\n",
      "Epoch [7550/10000], Loss: 151.03167724609375\n",
      "Epoch [7560/10000], Loss: 151.0182342529297\n",
      "Epoch [7570/10000], Loss: 151.01315307617188\n",
      "Epoch [7580/10000], Loss: 151.00791931152344\n",
      "Epoch [7590/10000], Loss: 151.0041961669922\n",
      "Epoch [7600/10000], Loss: 151.0019073486328\n",
      "Epoch [7610/10000], Loss: 151.01535034179688\n",
      "Epoch [7620/10000], Loss: 151.00062561035156\n",
      "Epoch [7630/10000], Loss: 150.9927978515625\n",
      "Epoch [7640/10000], Loss: 150.98922729492188\n",
      "Epoch [7650/10000], Loss: 150.98561096191406\n",
      "Epoch [7660/10000], Loss: 150.9824981689453\n",
      "Epoch [7670/10000], Loss: 150.97909545898438\n",
      "Epoch [7680/10000], Loss: 150.9762420654297\n",
      "Epoch [7690/10000], Loss: 150.97409057617188\n",
      "Epoch [7700/10000], Loss: 150.97052001953125\n",
      "Epoch [7710/10000], Loss: 150.97018432617188\n",
      "Epoch [7720/10000], Loss: 150.97531127929688\n",
      "Epoch [7730/10000], Loss: 150.96438598632812\n",
      "Epoch [7740/10000], Loss: 150.9591522216797\n",
      "Epoch [7750/10000], Loss: 150.95733642578125\n",
      "Epoch [7760/10000], Loss: 150.9547882080078\n",
      "Epoch [7770/10000], Loss: 150.9551544189453\n",
      "Epoch [7780/10000], Loss: 150.967041015625\n",
      "Epoch [7790/10000], Loss: 150.94459533691406\n",
      "Epoch [7800/10000], Loss: 150.94187927246094\n",
      "Epoch [7810/10000], Loss: 150.94192504882812\n",
      "Epoch [7820/10000], Loss: 150.93699645996094\n",
      "Epoch [7830/10000], Loss: 150.94253540039062\n",
      "Epoch [7840/10000], Loss: 150.93817138671875\n",
      "Epoch [7850/10000], Loss: 150.94325256347656\n",
      "Epoch [7860/10000], Loss: 150.92593383789062\n",
      "Epoch [7870/10000], Loss: 150.92408752441406\n",
      "Epoch [7880/10000], Loss: 150.9246826171875\n",
      "Epoch [7890/10000], Loss: 150.9210205078125\n",
      "Epoch [7900/10000], Loss: 150.9180145263672\n",
      "Epoch [7910/10000], Loss: 150.91436767578125\n",
      "Epoch [7920/10000], Loss: 150.91685485839844\n",
      "Epoch [7930/10000], Loss: 150.9174041748047\n",
      "Epoch [7940/10000], Loss: 150.90936279296875\n",
      "Epoch [7950/10000], Loss: 150.90628051757812\n",
      "Epoch [7960/10000], Loss: 150.9010772705078\n",
      "Epoch [7970/10000], Loss: 150.89805603027344\n",
      "Epoch [7980/10000], Loss: 150.8962860107422\n",
      "Epoch [7990/10000], Loss: 150.92311096191406\n",
      "Epoch [8000/10000], Loss: 150.889892578125\n",
      "Epoch [8010/10000], Loss: 150.88323974609375\n",
      "Epoch [8020/10000], Loss: 150.8813018798828\n",
      "Epoch [8030/10000], Loss: 150.8791961669922\n",
      "Epoch [8040/10000], Loss: 150.8771209716797\n",
      "Epoch [8050/10000], Loss: 150.87255859375\n",
      "Epoch [8060/10000], Loss: 150.87254333496094\n",
      "Epoch [8070/10000], Loss: 150.8753204345703\n",
      "Epoch [8080/10000], Loss: 150.8673095703125\n",
      "Epoch [8090/10000], Loss: 150.86363220214844\n",
      "Epoch [8100/10000], Loss: 150.88185119628906\n",
      "Epoch [8110/10000], Loss: 150.8743438720703\n",
      "Epoch [8120/10000], Loss: 150.86453247070312\n",
      "Epoch [8130/10000], Loss: 150.8558807373047\n",
      "Epoch [8140/10000], Loss: 150.85223388671875\n",
      "Epoch [8150/10000], Loss: 150.84730529785156\n",
      "Epoch [8160/10000], Loss: 150.85682678222656\n",
      "Epoch [8170/10000], Loss: 150.84637451171875\n",
      "Epoch [8180/10000], Loss: 150.8437957763672\n",
      "Epoch [8190/10000], Loss: 150.8428497314453\n",
      "Epoch [8200/10000], Loss: 150.83851623535156\n",
      "Epoch [8210/10000], Loss: 150.84262084960938\n",
      "Epoch [8220/10000], Loss: 150.8690948486328\n",
      "Epoch [8230/10000], Loss: 150.8388671875\n",
      "Epoch [8240/10000], Loss: 150.82833862304688\n",
      "Epoch [8250/10000], Loss: 150.82273864746094\n",
      "Epoch [8260/10000], Loss: 150.81973266601562\n",
      "Epoch [8270/10000], Loss: 150.81785583496094\n",
      "Epoch [8280/10000], Loss: 150.8151397705078\n",
      "Epoch [8290/10000], Loss: 150.81275939941406\n",
      "Epoch [8300/10000], Loss: 150.81065368652344\n",
      "Epoch [8310/10000], Loss: 150.8070831298828\n",
      "Epoch [8320/10000], Loss: 150.8065643310547\n",
      "Epoch [8330/10000], Loss: 150.8040771484375\n",
      "Epoch [8340/10000], Loss: 150.80418395996094\n",
      "Epoch [8350/10000], Loss: 150.86053466796875\n",
      "Epoch [8360/10000], Loss: 150.81483459472656\n",
      "Epoch [8370/10000], Loss: 150.79342651367188\n",
      "Epoch [8380/10000], Loss: 150.79075622558594\n",
      "Epoch [8390/10000], Loss: 150.78814697265625\n",
      "Epoch [8400/10000], Loss: 150.7849884033203\n",
      "Epoch [8410/10000], Loss: 150.781005859375\n",
      "Epoch [8420/10000], Loss: 150.77711486816406\n",
      "Epoch [8430/10000], Loss: 150.77468872070312\n",
      "Epoch [8440/10000], Loss: 150.77210998535156\n",
      "Epoch [8450/10000], Loss: 150.76991271972656\n",
      "Epoch [8460/10000], Loss: 150.76666259765625\n",
      "Epoch [8470/10000], Loss: 150.7652587890625\n",
      "Epoch [8480/10000], Loss: 150.78663635253906\n",
      "Epoch [8490/10000], Loss: 150.7590789794922\n",
      "Epoch [8500/10000], Loss: 150.75619506835938\n",
      "Epoch [8510/10000], Loss: 150.7547149658203\n",
      "Epoch [8520/10000], Loss: 150.75054931640625\n",
      "Epoch [8530/10000], Loss: 150.74716186523438\n",
      "Epoch [8540/10000], Loss: 150.74522399902344\n",
      "Epoch [8550/10000], Loss: 150.74266052246094\n",
      "Epoch [8560/10000], Loss: 150.74029541015625\n",
      "Epoch [8570/10000], Loss: 150.74893188476562\n",
      "Epoch [8580/10000], Loss: 150.74435424804688\n",
      "Epoch [8590/10000], Loss: 150.7362518310547\n",
      "Epoch [8600/10000], Loss: 150.72940063476562\n",
      "Epoch [8610/10000], Loss: 150.73077392578125\n",
      "Epoch [8620/10000], Loss: 150.72640991210938\n",
      "Epoch [8630/10000], Loss: 150.7315216064453\n",
      "Epoch [8640/10000], Loss: 150.74378967285156\n",
      "Epoch [8650/10000], Loss: 150.7205352783203\n",
      "Epoch [8660/10000], Loss: 150.7305145263672\n",
      "Epoch [8670/10000], Loss: 150.71755981445312\n",
      "Epoch [8680/10000], Loss: 150.71371459960938\n",
      "Epoch [8690/10000], Loss: 150.7114715576172\n",
      "Epoch [8700/10000], Loss: 150.70855712890625\n",
      "Epoch [8710/10000], Loss: 150.72000122070312\n",
      "Epoch [8720/10000], Loss: 150.7098846435547\n",
      "Epoch [8730/10000], Loss: 150.70530700683594\n",
      "Epoch [8740/10000], Loss: 150.6996307373047\n",
      "Epoch [8750/10000], Loss: 150.69435119628906\n",
      "Epoch [8760/10000], Loss: 150.6904754638672\n",
      "Epoch [8770/10000], Loss: 150.68792724609375\n",
      "Epoch [8780/10000], Loss: 150.68663024902344\n",
      "Epoch [8790/10000], Loss: 150.68507385253906\n",
      "Epoch [8800/10000], Loss: 150.7034912109375\n",
      "Epoch [8810/10000], Loss: 150.67947387695312\n",
      "Epoch [8820/10000], Loss: 150.67849731445312\n",
      "Epoch [8830/10000], Loss: 150.6791229248047\n",
      "Epoch [8840/10000], Loss: 150.6730499267578\n",
      "Epoch [8850/10000], Loss: 150.67105102539062\n",
      "Epoch [8860/10000], Loss: 150.67144775390625\n",
      "Epoch [8870/10000], Loss: 150.66595458984375\n",
      "Epoch [8880/10000], Loss: 150.66323852539062\n",
      "Epoch [8890/10000], Loss: 150.66123962402344\n",
      "Epoch [8900/10000], Loss: 150.66477966308594\n",
      "Epoch [8910/10000], Loss: 150.69564819335938\n",
      "Epoch [8920/10000], Loss: 150.6702117919922\n",
      "Epoch [8930/10000], Loss: 150.65956115722656\n",
      "Epoch [8940/10000], Loss: 150.65896606445312\n",
      "Epoch [8950/10000], Loss: 150.65147399902344\n",
      "Epoch [8960/10000], Loss: 150.65093994140625\n",
      "Epoch [8970/10000], Loss: 150.6549072265625\n",
      "Epoch [8980/10000], Loss: 150.64630126953125\n",
      "Epoch [8990/10000], Loss: 150.64437866210938\n",
      "Epoch [9000/10000], Loss: 150.6417236328125\n",
      "Epoch [9010/10000], Loss: 150.6424102783203\n",
      "Epoch [9020/10000], Loss: 150.6517791748047\n",
      "Epoch [9030/10000], Loss: 150.63803100585938\n",
      "Epoch [9040/10000], Loss: 150.63534545898438\n",
      "Epoch [9050/10000], Loss: 150.62904357910156\n",
      "Epoch [9060/10000], Loss: 150.6382598876953\n",
      "Epoch [9070/10000], Loss: 150.6427001953125\n",
      "Epoch [9080/10000], Loss: 150.64109802246094\n",
      "Epoch [9090/10000], Loss: 150.64573669433594\n",
      "Epoch [9100/10000], Loss: 150.6180419921875\n",
      "Epoch [9110/10000], Loss: 150.6171875\n",
      "Epoch [9120/10000], Loss: 150.61456298828125\n",
      "Epoch [9130/10000], Loss: 150.6107177734375\n",
      "Epoch [9140/10000], Loss: 150.6099090576172\n",
      "Epoch [9150/10000], Loss: 150.60838317871094\n",
      "Epoch [9160/10000], Loss: 150.6170196533203\n",
      "Epoch [9170/10000], Loss: 150.61004638671875\n",
      "Epoch [9180/10000], Loss: 150.60877990722656\n",
      "Epoch [9190/10000], Loss: 150.60020446777344\n",
      "Epoch [9200/10000], Loss: 150.5985870361328\n",
      "Epoch [9210/10000], Loss: 150.59603881835938\n",
      "Epoch [9220/10000], Loss: 150.5936737060547\n",
      "Epoch [9230/10000], Loss: 150.59185791015625\n",
      "Epoch [9240/10000], Loss: 150.59947204589844\n",
      "Epoch [9250/10000], Loss: 150.60830688476562\n",
      "Epoch [9260/10000], Loss: 150.59153747558594\n",
      "Epoch [9270/10000], Loss: 150.5841064453125\n",
      "Epoch [9280/10000], Loss: 150.5810546875\n",
      "Epoch [9290/10000], Loss: 150.579345703125\n",
      "Epoch [9300/10000], Loss: 150.57550048828125\n",
      "Epoch [9310/10000], Loss: 150.57545471191406\n",
      "Epoch [9320/10000], Loss: 150.5752410888672\n",
      "Epoch [9330/10000], Loss: 150.5707244873047\n",
      "Epoch [9340/10000], Loss: 150.57058715820312\n",
      "Epoch [9350/10000], Loss: 150.576416015625\n",
      "Epoch [9360/10000], Loss: 150.5729217529297\n",
      "Epoch [9370/10000], Loss: 150.5654754638672\n",
      "Epoch [9380/10000], Loss: 150.5665740966797\n",
      "Epoch [9390/10000], Loss: 150.5597686767578\n",
      "Epoch [9400/10000], Loss: 150.55958557128906\n",
      "Epoch [9410/10000], Loss: 150.5579071044922\n",
      "Epoch [9420/10000], Loss: 150.5671844482422\n",
      "Epoch [9430/10000], Loss: 150.5688018798828\n",
      "Epoch [9440/10000], Loss: 150.55813598632812\n",
      "Epoch [9450/10000], Loss: 150.5496368408203\n",
      "Epoch [9460/10000], Loss: 150.54693603515625\n",
      "Epoch [9470/10000], Loss: 150.54501342773438\n",
      "Epoch [9480/10000], Loss: 150.54090881347656\n",
      "Epoch [9490/10000], Loss: 150.54380798339844\n",
      "Epoch [9500/10000], Loss: 150.5396270751953\n",
      "Epoch [9510/10000], Loss: 150.53622436523438\n",
      "Epoch [9520/10000], Loss: 150.53932189941406\n",
      "Epoch [9530/10000], Loss: 150.55990600585938\n",
      "Epoch [9540/10000], Loss: 150.531494140625\n",
      "Epoch [9550/10000], Loss: 150.5288543701172\n",
      "Epoch [9560/10000], Loss: 150.52671813964844\n",
      "Epoch [9570/10000], Loss: 150.5249481201172\n",
      "Epoch [9580/10000], Loss: 150.52650451660156\n",
      "Epoch [9590/10000], Loss: 150.52032470703125\n",
      "Epoch [9600/10000], Loss: 150.52267456054688\n",
      "Epoch [9610/10000], Loss: 150.5261688232422\n",
      "Epoch [9620/10000], Loss: 150.52952575683594\n",
      "Epoch [9630/10000], Loss: 150.51844787597656\n",
      "Epoch [9640/10000], Loss: 150.51161193847656\n",
      "Epoch [9650/10000], Loss: 150.5099334716797\n",
      "Epoch [9660/10000], Loss: 150.50772094726562\n",
      "Epoch [9670/10000], Loss: 150.50680541992188\n",
      "Epoch [9680/10000], Loss: 150.5064697265625\n",
      "Epoch [9690/10000], Loss: 150.51736450195312\n",
      "Epoch [9700/10000], Loss: 150.5118865966797\n",
      "Epoch [9710/10000], Loss: 150.5048370361328\n",
      "Epoch [9720/10000], Loss: 150.4966583251953\n",
      "Epoch [9730/10000], Loss: 150.49598693847656\n",
      "Epoch [9740/10000], Loss: 150.49432373046875\n",
      "Epoch [9750/10000], Loss: 150.49185180664062\n",
      "Epoch [9760/10000], Loss: 150.49012756347656\n",
      "Epoch [9770/10000], Loss: 150.49160766601562\n",
      "Epoch [9780/10000], Loss: 150.48703002929688\n",
      "Epoch [9790/10000], Loss: 150.4837646484375\n",
      "Epoch [9800/10000], Loss: 150.48838806152344\n",
      "Epoch [9810/10000], Loss: 150.5177459716797\n",
      "Epoch [9820/10000], Loss: 150.49359130859375\n",
      "Epoch [9830/10000], Loss: 150.48020935058594\n",
      "Epoch [9840/10000], Loss: 150.47549438476562\n",
      "Epoch [9850/10000], Loss: 150.47415161132812\n",
      "Epoch [9860/10000], Loss: 150.47438049316406\n",
      "Epoch [9870/10000], Loss: 150.47032165527344\n",
      "Epoch [9880/10000], Loss: 150.46617126464844\n",
      "Epoch [9890/10000], Loss: 150.46693420410156\n",
      "Epoch [9900/10000], Loss: 150.4790496826172\n",
      "Epoch [9910/10000], Loss: 150.46192932128906\n",
      "Epoch [9920/10000], Loss: 150.46424865722656\n",
      "Epoch [9930/10000], Loss: 150.45919799804688\n",
      "Epoch [9940/10000], Loss: 150.45616149902344\n",
      "Epoch [9950/10000], Loss: 150.45506286621094\n",
      "Epoch [9960/10000], Loss: 150.46109008789062\n",
      "Epoch [9970/10000], Loss: 150.4718780517578\n",
      "Epoch [9980/10000], Loss: 150.45591735839844\n",
      "Epoch [9990/10000], Loss: 150.44801330566406\n",
      "Epoch [10000/10000], Loss: 150.4474639892578\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs.squeeze(), y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "081545db-303e-4eec-81be-6051012e296c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error with Neural Network: 149.9329547691226\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor).squeeze().numpy()\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    print(f'Mean Squared Error with Neural Network: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7699be8a-4651-454c-bb9f-2c01ae3b9fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
