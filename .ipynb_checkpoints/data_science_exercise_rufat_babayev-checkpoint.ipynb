{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "921b16b4",
   "metadata": {},
   "source": [
    "# Loss for Demand Predictions of an Online Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba506c88",
   "metadata": {},
   "source": [
    "You are tasked with forecasting the daily demand of an online retailer over a certain time period based on\n",
    "historical sales data. In addition to the sales data you are also provided with the stock (inventory) that was\n",
    "available for sale on each day. Therefore, the stock information provides an upper limit for the possible sales\n",
    "on each day. This small task is not concerned with the development of the model itself but only focuses on\n",
    "developing and implementing a tailor-suited loss function for this forecasting problem.\n",
    "\n",
    "- Develop a loss function in PyTorch based on the code snippet below.\n",
    "- Explain your choice and why it is a good fit for the problem at hand.\n",
    "- Evaluate the loss functionâ€™s performance with the provided dummy model. Adjust the code where\n",
    "needed to arrive at conclusions. Summarize and explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed77dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b941c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input tensors for stocks and sales\n",
    "n = 1000\n",
    "torch.manual_seed(0)\n",
    "stocks = torch.randint(0, 10, (n,))\n",
    "demand = torch.poisson(torch.ones(n) * 2.0)\n",
    "sales = torch.min(demand, stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf345f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(stocks)\n",
    "# display(demand)\n",
    "# display(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b55c73",
   "metadata": {},
   "source": [
    "```diff\n",
    "@@ Given the problem statement, one possible approach to defining a loss function would be to penalize both overestimations  and underestimations of demand. In particular, we want to avoid overestimations,  as they might lead to wastage of resources (since stock exceeding demand might go unused), and we also want to avoid underestimations, as they might lead to lost sales opportunities.\n",
    "\n",
    "@@ One potential loss function to use could be a weighted mean absolute error (WMAE), where we assign different weights to overestimations and underestimations.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024e7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def forecast_loss(predictions, sales, stocks):\n",
    "    sales = torch.min(sales, stocks)\n",
    "    overestimation = torch.clamp(predictions - sales, min=0)\n",
    "    underestimation = torch.clamp(sales - predictions, min=0)\n",
    "    \n",
    "    # Set different weights for overestimation and underestimation\n",
    "    w1, w2 = 0.6, 0.4\n",
    "    loss = w1 * torch.mean(overestimation) + w2 * torch.mean(underestimation)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b2bf59",
   "metadata": {},
   "source": [
    "```diff\n",
    "\n",
    "@@ In the forecast_loss function, we separate the overestimation and underestimation by using torch.clamp. Then we calculate the mean of each part and sum them with different weights. Here, we assign more weight to overestimation (w1=0.6), as we might consider overestimation as more undesirable.\n",
    "\n",
    "@@ When training the model, the loss function will guide the model to forecast the demand with the least overall error. The model will make a trade-off between underestimation and overestimation according to the weights we have set.\n",
    "\n",
    "@@ This loss function is useful because it allows us to specify how much we value avoiding overestimation versus avoiding underestimation, which can be tailored to the specific business context. For example, if the cost of overestimation is significantly higher than the cost of underestimation (perhaps because of storage costs, or because unsold items perish), then we would want to set a higher weight for the overestimation term.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f88613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanModel, self).__init__()\n",
    "        self.mean = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "    def forward(self, n):\n",
    "        return self.mean * torch.ones(n) # this a liner model which might not perform better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e66dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6617])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1) # generates a negative value when multiplied by torch.ones(n), can produce negative outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a374c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and optimize using gradient descent\n",
    "model = MeanModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # tested lr rate of 0.001, after 20000 iterations loss stays the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece22ed",
   "metadata": {},
   "source": [
    "```diff\n",
    "@@ The performance of the loss function can be evaluated by running the training loop and checking whether the loss decreases over time, indicating that the model is learning to make better forecasts. Note that in order to make this a fair test, we might need to ensure that the training data includes a variety of situations, including both times when demand exceeds stock, and times when stock exceeds demand.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e61ac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times when demand exceeds stock:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  3],\n",
       "        [  4],\n",
       "        [  9],\n",
       "        [ 10],\n",
       "        [ 19],\n",
       "        [ 29],\n",
       "        [ 30],\n",
       "        [ 33],\n",
       "        [ 36],\n",
       "        [ 44],\n",
       "        [ 48],\n",
       "        [ 50],\n",
       "        [ 54],\n",
       "        [ 55],\n",
       "        [ 65],\n",
       "        [ 67],\n",
       "        [ 72],\n",
       "        [ 73],\n",
       "        [ 78],\n",
       "        [ 79],\n",
       "        [ 85],\n",
       "        [ 92],\n",
       "        [105],\n",
       "        [107],\n",
       "        [117],\n",
       "        [124],\n",
       "        [129],\n",
       "        [136],\n",
       "        [138],\n",
       "        [143],\n",
       "        [145],\n",
       "        [150],\n",
       "        [153],\n",
       "        [158],\n",
       "        [160],\n",
       "        [165],\n",
       "        [174],\n",
       "        [176],\n",
       "        [179],\n",
       "        [185],\n",
       "        [194],\n",
       "        [196],\n",
       "        [209],\n",
       "        [222],\n",
       "        [224],\n",
       "        [228],\n",
       "        [229],\n",
       "        [240],\n",
       "        [242],\n",
       "        [246],\n",
       "        [248],\n",
       "        [263],\n",
       "        [270],\n",
       "        [274],\n",
       "        [292],\n",
       "        [294],\n",
       "        [298],\n",
       "        [300],\n",
       "        [306],\n",
       "        [317],\n",
       "        [318],\n",
       "        [324],\n",
       "        [325],\n",
       "        [330],\n",
       "        [331],\n",
       "        [339],\n",
       "        [342],\n",
       "        [345],\n",
       "        [346],\n",
       "        [351],\n",
       "        [354],\n",
       "        [362],\n",
       "        [365],\n",
       "        [366],\n",
       "        [367],\n",
       "        [373],\n",
       "        [380],\n",
       "        [381],\n",
       "        [383],\n",
       "        [391],\n",
       "        [395],\n",
       "        [397],\n",
       "        [409],\n",
       "        [413],\n",
       "        [416],\n",
       "        [419],\n",
       "        [422],\n",
       "        [427],\n",
       "        [440],\n",
       "        [455],\n",
       "        [456],\n",
       "        [458],\n",
       "        [459],\n",
       "        [460],\n",
       "        [465],\n",
       "        [468],\n",
       "        [472],\n",
       "        [473],\n",
       "        [474],\n",
       "        [485],\n",
       "        [488],\n",
       "        [498],\n",
       "        [501],\n",
       "        [504],\n",
       "        [505],\n",
       "        [525],\n",
       "        [526],\n",
       "        [528],\n",
       "        [542],\n",
       "        [545],\n",
       "        [546],\n",
       "        [550],\n",
       "        [551],\n",
       "        [559],\n",
       "        [573],\n",
       "        [582],\n",
       "        [583],\n",
       "        [586],\n",
       "        [593],\n",
       "        [604],\n",
       "        [615],\n",
       "        [616],\n",
       "        [622],\n",
       "        [623],\n",
       "        [624],\n",
       "        [631],\n",
       "        [634],\n",
       "        [643],\n",
       "        [644],\n",
       "        [653],\n",
       "        [655],\n",
       "        [656],\n",
       "        [660],\n",
       "        [665],\n",
       "        [668],\n",
       "        [673],\n",
       "        [684],\n",
       "        [693],\n",
       "        [697],\n",
       "        [701],\n",
       "        [704],\n",
       "        [711],\n",
       "        [715],\n",
       "        [723],\n",
       "        [726],\n",
       "        [728],\n",
       "        [734],\n",
       "        [736],\n",
       "        [740],\n",
       "        [742],\n",
       "        [754],\n",
       "        [756],\n",
       "        [776],\n",
       "        [778],\n",
       "        [787],\n",
       "        [788],\n",
       "        [801],\n",
       "        [804],\n",
       "        [809],\n",
       "        [810],\n",
       "        [813],\n",
       "        [816],\n",
       "        [822],\n",
       "        [824],\n",
       "        [831],\n",
       "        [834],\n",
       "        [835],\n",
       "        [839],\n",
       "        [840],\n",
       "        [841],\n",
       "        [842],\n",
       "        [845],\n",
       "        [846],\n",
       "        [847],\n",
       "        [854],\n",
       "        [859],\n",
       "        [860],\n",
       "        [870],\n",
       "        [871],\n",
       "        [876],\n",
       "        [877],\n",
       "        [878],\n",
       "        [882],\n",
       "        [883],\n",
       "        [902],\n",
       "        [903],\n",
       "        [925],\n",
       "        [929],\n",
       "        [930],\n",
       "        [934],\n",
       "        [941],\n",
       "        [947],\n",
       "        [949],\n",
       "        [952],\n",
       "        [953],\n",
       "        [955],\n",
       "        [968],\n",
       "        [969],\n",
       "        [971],\n",
       "        [974],\n",
       "        [981],\n",
       "        [983],\n",
       "        [984],\n",
       "        [985],\n",
       "        [995],\n",
       "        [999]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Times when stock exceeds demand:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0],\n",
       "        [  1],\n",
       "        [  2],\n",
       "        [  5],\n",
       "        [  6],\n",
       "        [  7],\n",
       "        [  8],\n",
       "        [ 11],\n",
       "        [ 12],\n",
       "        [ 13],\n",
       "        [ 14],\n",
       "        [ 15],\n",
       "        [ 16],\n",
       "        [ 17],\n",
       "        [ 18],\n",
       "        [ 20],\n",
       "        [ 21],\n",
       "        [ 23],\n",
       "        [ 24],\n",
       "        [ 26],\n",
       "        [ 27],\n",
       "        [ 28],\n",
       "        [ 32],\n",
       "        [ 34],\n",
       "        [ 35],\n",
       "        [ 37],\n",
       "        [ 39],\n",
       "        [ 40],\n",
       "        [ 41],\n",
       "        [ 42],\n",
       "        [ 43],\n",
       "        [ 45],\n",
       "        [ 46],\n",
       "        [ 47],\n",
       "        [ 51],\n",
       "        [ 53],\n",
       "        [ 56],\n",
       "        [ 57],\n",
       "        [ 58],\n",
       "        [ 59],\n",
       "        [ 60],\n",
       "        [ 61],\n",
       "        [ 62],\n",
       "        [ 63],\n",
       "        [ 64],\n",
       "        [ 66],\n",
       "        [ 68],\n",
       "        [ 70],\n",
       "        [ 71],\n",
       "        [ 74],\n",
       "        [ 75],\n",
       "        [ 76],\n",
       "        [ 77],\n",
       "        [ 81],\n",
       "        [ 82],\n",
       "        [ 83],\n",
       "        [ 84],\n",
       "        [ 86],\n",
       "        [ 87],\n",
       "        [ 88],\n",
       "        [ 89],\n",
       "        [ 90],\n",
       "        [ 93],\n",
       "        [ 94],\n",
       "        [ 95],\n",
       "        [ 96],\n",
       "        [ 97],\n",
       "        [ 98],\n",
       "        [100],\n",
       "        [101],\n",
       "        [102],\n",
       "        [103],\n",
       "        [104],\n",
       "        [106],\n",
       "        [108],\n",
       "        [109],\n",
       "        [110],\n",
       "        [111],\n",
       "        [113],\n",
       "        [115],\n",
       "        [116],\n",
       "        [118],\n",
       "        [119],\n",
       "        [120],\n",
       "        [121],\n",
       "        [122],\n",
       "        [123],\n",
       "        [126],\n",
       "        [127],\n",
       "        [128],\n",
       "        [130],\n",
       "        [132],\n",
       "        [133],\n",
       "        [134],\n",
       "        [137],\n",
       "        [139],\n",
       "        [140],\n",
       "        [141],\n",
       "        [146],\n",
       "        [147],\n",
       "        [148],\n",
       "        [149],\n",
       "        [151],\n",
       "        [152],\n",
       "        [154],\n",
       "        [155],\n",
       "        [159],\n",
       "        [161],\n",
       "        [162],\n",
       "        [163],\n",
       "        [164],\n",
       "        [166],\n",
       "        [167],\n",
       "        [168],\n",
       "        [170],\n",
       "        [172],\n",
       "        [173],\n",
       "        [175],\n",
       "        [177],\n",
       "        [178],\n",
       "        [181],\n",
       "        [182],\n",
       "        [184],\n",
       "        [186],\n",
       "        [187],\n",
       "        [188],\n",
       "        [189],\n",
       "        [190],\n",
       "        [191],\n",
       "        [192],\n",
       "        [193],\n",
       "        [195],\n",
       "        [197],\n",
       "        [198],\n",
       "        [199],\n",
       "        [200],\n",
       "        [201],\n",
       "        [202],\n",
       "        [203],\n",
       "        [204],\n",
       "        [205],\n",
       "        [206],\n",
       "        [207],\n",
       "        [210],\n",
       "        [212],\n",
       "        [213],\n",
       "        [214],\n",
       "        [215],\n",
       "        [216],\n",
       "        [217],\n",
       "        [218],\n",
       "        [219],\n",
       "        [220],\n",
       "        [221],\n",
       "        [223],\n",
       "        [225],\n",
       "        [227],\n",
       "        [230],\n",
       "        [231],\n",
       "        [232],\n",
       "        [233],\n",
       "        [235],\n",
       "        [236],\n",
       "        [237],\n",
       "        [238],\n",
       "        [239],\n",
       "        [241],\n",
       "        [243],\n",
       "        [244],\n",
       "        [245],\n",
       "        [247],\n",
       "        [249],\n",
       "        [250],\n",
       "        [251],\n",
       "        [252],\n",
       "        [253],\n",
       "        [254],\n",
       "        [255],\n",
       "        [256],\n",
       "        [258],\n",
       "        [260],\n",
       "        [261],\n",
       "        [262],\n",
       "        [264],\n",
       "        [265],\n",
       "        [266],\n",
       "        [267],\n",
       "        [268],\n",
       "        [269],\n",
       "        [271],\n",
       "        [272],\n",
       "        [273],\n",
       "        [275],\n",
       "        [276],\n",
       "        [278],\n",
       "        [279],\n",
       "        [280],\n",
       "        [282],\n",
       "        [283],\n",
       "        [284],\n",
       "        [285],\n",
       "        [286],\n",
       "        [287],\n",
       "        [288],\n",
       "        [289],\n",
       "        [290],\n",
       "        [291],\n",
       "        [293],\n",
       "        [296],\n",
       "        [297],\n",
       "        [299],\n",
       "        [301],\n",
       "        [302],\n",
       "        [303],\n",
       "        [304],\n",
       "        [305],\n",
       "        [307],\n",
       "        [308],\n",
       "        [309],\n",
       "        [310],\n",
       "        [311],\n",
       "        [313],\n",
       "        [314],\n",
       "        [315],\n",
       "        [316],\n",
       "        [319],\n",
       "        [320],\n",
       "        [321],\n",
       "        [322],\n",
       "        [323],\n",
       "        [326],\n",
       "        [327],\n",
       "        [329],\n",
       "        [332],\n",
       "        [333],\n",
       "        [334],\n",
       "        [335],\n",
       "        [336],\n",
       "        [338],\n",
       "        [341],\n",
       "        [343],\n",
       "        [344],\n",
       "        [347],\n",
       "        [348],\n",
       "        [349],\n",
       "        [350],\n",
       "        [352],\n",
       "        [353],\n",
       "        [355],\n",
       "        [356],\n",
       "        [357],\n",
       "        [358],\n",
       "        [359],\n",
       "        [360],\n",
       "        [361],\n",
       "        [363],\n",
       "        [364],\n",
       "        [368],\n",
       "        [369],\n",
       "        [370],\n",
       "        [371],\n",
       "        [372],\n",
       "        [374],\n",
       "        [375],\n",
       "        [376],\n",
       "        [378],\n",
       "        [379],\n",
       "        [384],\n",
       "        [385],\n",
       "        [386],\n",
       "        [387],\n",
       "        [392],\n",
       "        [393],\n",
       "        [394],\n",
       "        [396],\n",
       "        [399],\n",
       "        [400],\n",
       "        [401],\n",
       "        [402],\n",
       "        [403],\n",
       "        [404],\n",
       "        [405],\n",
       "        [407],\n",
       "        [408],\n",
       "        [410],\n",
       "        [411],\n",
       "        [412],\n",
       "        [414],\n",
       "        [415],\n",
       "        [417],\n",
       "        [418],\n",
       "        [420],\n",
       "        [421],\n",
       "        [424],\n",
       "        [425],\n",
       "        [429],\n",
       "        [430],\n",
       "        [431],\n",
       "        [432],\n",
       "        [433],\n",
       "        [434],\n",
       "        [435],\n",
       "        [436],\n",
       "        [437],\n",
       "        [438],\n",
       "        [439],\n",
       "        [441],\n",
       "        [442],\n",
       "        [443],\n",
       "        [444],\n",
       "        [445],\n",
       "        [446],\n",
       "        [447],\n",
       "        [448],\n",
       "        [449],\n",
       "        [451],\n",
       "        [452],\n",
       "        [454],\n",
       "        [457],\n",
       "        [461],\n",
       "        [462],\n",
       "        [463],\n",
       "        [464],\n",
       "        [466],\n",
       "        [469],\n",
       "        [470],\n",
       "        [471],\n",
       "        [475],\n",
       "        [476],\n",
       "        [477],\n",
       "        [478],\n",
       "        [479],\n",
       "        [480],\n",
       "        [481],\n",
       "        [482],\n",
       "        [483],\n",
       "        [484],\n",
       "        [487],\n",
       "        [489],\n",
       "        [490],\n",
       "        [491],\n",
       "        [492],\n",
       "        [493],\n",
       "        [494],\n",
       "        [495],\n",
       "        [497],\n",
       "        [499],\n",
       "        [500],\n",
       "        [502],\n",
       "        [503],\n",
       "        [506],\n",
       "        [508],\n",
       "        [510],\n",
       "        [511],\n",
       "        [512],\n",
       "        [513],\n",
       "        [514],\n",
       "        [516],\n",
       "        [517],\n",
       "        [519],\n",
       "        [520],\n",
       "        [523],\n",
       "        [524],\n",
       "        [527],\n",
       "        [530],\n",
       "        [531],\n",
       "        [532],\n",
       "        [533],\n",
       "        [534],\n",
       "        [535],\n",
       "        [536],\n",
       "        [537],\n",
       "        [540],\n",
       "        [541],\n",
       "        [543],\n",
       "        [544],\n",
       "        [547],\n",
       "        [548],\n",
       "        [549],\n",
       "        [552],\n",
       "        [553],\n",
       "        [554],\n",
       "        [555],\n",
       "        [556],\n",
       "        [557],\n",
       "        [558],\n",
       "        [560],\n",
       "        [561],\n",
       "        [562],\n",
       "        [563],\n",
       "        [564],\n",
       "        [565],\n",
       "        [566],\n",
       "        [567],\n",
       "        [568],\n",
       "        [569],\n",
       "        [570],\n",
       "        [571],\n",
       "        [572],\n",
       "        [574],\n",
       "        [575],\n",
       "        [576],\n",
       "        [578],\n",
       "        [579],\n",
       "        [580],\n",
       "        [581],\n",
       "        [584],\n",
       "        [585],\n",
       "        [587],\n",
       "        [588],\n",
       "        [589],\n",
       "        [592],\n",
       "        [594],\n",
       "        [595],\n",
       "        [596],\n",
       "        [598],\n",
       "        [599],\n",
       "        [600],\n",
       "        [602],\n",
       "        [603],\n",
       "        [606],\n",
       "        [608],\n",
       "        [609],\n",
       "        [610],\n",
       "        [611],\n",
       "        [613],\n",
       "        [617],\n",
       "        [618],\n",
       "        [619],\n",
       "        [620],\n",
       "        [621],\n",
       "        [625],\n",
       "        [626],\n",
       "        [628],\n",
       "        [629],\n",
       "        [630],\n",
       "        [632],\n",
       "        [633],\n",
       "        [635],\n",
       "        [636],\n",
       "        [637],\n",
       "        [638],\n",
       "        [639],\n",
       "        [640],\n",
       "        [642],\n",
       "        [645],\n",
       "        [646],\n",
       "        [647],\n",
       "        [648],\n",
       "        [649],\n",
       "        [651],\n",
       "        [652],\n",
       "        [657],\n",
       "        [659],\n",
       "        [661],\n",
       "        [662],\n",
       "        [663],\n",
       "        [664],\n",
       "        [666],\n",
       "        [667],\n",
       "        [669],\n",
       "        [670],\n",
       "        [671],\n",
       "        [672],\n",
       "        [674],\n",
       "        [675],\n",
       "        [676],\n",
       "        [677],\n",
       "        [678],\n",
       "        [679],\n",
       "        [680],\n",
       "        [681],\n",
       "        [682],\n",
       "        [685],\n",
       "        [686],\n",
       "        [687],\n",
       "        [688],\n",
       "        [689],\n",
       "        [691],\n",
       "        [694],\n",
       "        [696],\n",
       "        [698],\n",
       "        [699],\n",
       "        [700],\n",
       "        [702],\n",
       "        [703],\n",
       "        [705],\n",
       "        [706],\n",
       "        [707],\n",
       "        [708],\n",
       "        [709],\n",
       "        [710],\n",
       "        [712],\n",
       "        [713],\n",
       "        [714],\n",
       "        [716],\n",
       "        [717],\n",
       "        [718],\n",
       "        [719],\n",
       "        [720],\n",
       "        [721],\n",
       "        [722],\n",
       "        [724],\n",
       "        [725],\n",
       "        [727],\n",
       "        [730],\n",
       "        [731],\n",
       "        [732],\n",
       "        [733],\n",
       "        [735],\n",
       "        [737],\n",
       "        [738],\n",
       "        [739],\n",
       "        [741],\n",
       "        [743],\n",
       "        [744],\n",
       "        [745],\n",
       "        [746],\n",
       "        [748],\n",
       "        [749],\n",
       "        [750],\n",
       "        [751],\n",
       "        [752],\n",
       "        [753],\n",
       "        [755],\n",
       "        [757],\n",
       "        [758],\n",
       "        [761],\n",
       "        [764],\n",
       "        [765],\n",
       "        [767],\n",
       "        [768],\n",
       "        [769],\n",
       "        [770],\n",
       "        [771],\n",
       "        [772],\n",
       "        [773],\n",
       "        [774],\n",
       "        [775],\n",
       "        [777],\n",
       "        [779],\n",
       "        [780],\n",
       "        [781],\n",
       "        [782],\n",
       "        [783],\n",
       "        [785],\n",
       "        [786],\n",
       "        [789],\n",
       "        [790],\n",
       "        [791],\n",
       "        [792],\n",
       "        [794],\n",
       "        [796],\n",
       "        [797],\n",
       "        [798],\n",
       "        [799],\n",
       "        [800],\n",
       "        [802],\n",
       "        [803],\n",
       "        [805],\n",
       "        [807],\n",
       "        [808],\n",
       "        [811],\n",
       "        [812],\n",
       "        [814],\n",
       "        [815],\n",
       "        [817],\n",
       "        [818],\n",
       "        [819],\n",
       "        [820],\n",
       "        [821],\n",
       "        [823],\n",
       "        [825],\n",
       "        [827],\n",
       "        [828],\n",
       "        [829],\n",
       "        [830],\n",
       "        [832],\n",
       "        [833],\n",
       "        [838],\n",
       "        [843],\n",
       "        [844],\n",
       "        [848],\n",
       "        [849],\n",
       "        [851],\n",
       "        [852],\n",
       "        [853],\n",
       "        [855],\n",
       "        [856],\n",
       "        [858],\n",
       "        [861],\n",
       "        [862],\n",
       "        [863],\n",
       "        [864],\n",
       "        [865],\n",
       "        [866],\n",
       "        [867],\n",
       "        [868],\n",
       "        [869],\n",
       "        [872],\n",
       "        [873],\n",
       "        [874],\n",
       "        [879],\n",
       "        [880],\n",
       "        [884],\n",
       "        [886],\n",
       "        [887],\n",
       "        [888],\n",
       "        [889],\n",
       "        [890],\n",
       "        [892],\n",
       "        [893],\n",
       "        [894],\n",
       "        [895],\n",
       "        [896],\n",
       "        [897],\n",
       "        [898],\n",
       "        [899],\n",
       "        [900],\n",
       "        [901],\n",
       "        [904],\n",
       "        [905],\n",
       "        [906],\n",
       "        [907],\n",
       "        [908],\n",
       "        [909],\n",
       "        [910],\n",
       "        [911],\n",
       "        [912],\n",
       "        [915],\n",
       "        [916],\n",
       "        [917],\n",
       "        [918],\n",
       "        [919],\n",
       "        [920],\n",
       "        [921],\n",
       "        [923],\n",
       "        [924],\n",
       "        [926],\n",
       "        [927],\n",
       "        [928],\n",
       "        [932],\n",
       "        [935],\n",
       "        [936],\n",
       "        [937],\n",
       "        [938],\n",
       "        [939],\n",
       "        [942],\n",
       "        [943],\n",
       "        [944],\n",
       "        [945],\n",
       "        [946],\n",
       "        [948],\n",
       "        [950],\n",
       "        [951],\n",
       "        [957],\n",
       "        [958],\n",
       "        [959],\n",
       "        [960],\n",
       "        [961],\n",
       "        [962],\n",
       "        [964],\n",
       "        [965],\n",
       "        [966],\n",
       "        [967],\n",
       "        [970],\n",
       "        [973],\n",
       "        [975],\n",
       "        [976],\n",
       "        [977],\n",
       "        [978],\n",
       "        [979],\n",
       "        [980],\n",
       "        [982],\n",
       "        [987],\n",
       "        [988],\n",
       "        [989],\n",
       "        [991],\n",
       "        [992],\n",
       "        [993],\n",
       "        [994],\n",
       "        [996],\n",
       "        [997],\n",
       "        [998]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print when demand exceeds stock\n",
    "excess_demand = demand > stocks\n",
    "print(\"Times when demand exceeds stock:\")\n",
    "display(torch.nonzero(excess_demand))\n",
    "\n",
    "# Print when stock exceeds demand\n",
    "excess_stock = stocks > demand\n",
    "print(\"\\nTimes when stock exceeds demand:\")\n",
    "display(torch.nonzero(excess_stock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f136d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 Loss: 0.6275261044502258\n",
      "Epoch: 199 Loss: 0.5999700427055359\n",
      "Epoch: 299 Loss: 0.5724143385887146\n",
      "Epoch: 399 Loss: 0.5448583960533142\n",
      "Epoch: 499 Loss: 0.5173026323318481\n",
      "Epoch: 599 Loss: 0.4897467792034149\n",
      "Epoch: 699 Loss: 0.4832867383956909\n",
      "Epoch: 799 Loss: 0.4833013415336609\n",
      "Epoch: 899 Loss: 0.48331600427627563\n",
      "Epoch: 999 Loss: 0.483330637216568\n",
      "Epoch: 1099 Loss: 0.48334527015686035\n",
      "Epoch: 1199 Loss: 0.48335981369018555\n",
      "Epoch: 1299 Loss: 0.4833745062351227\n",
      "Epoch: 1399 Loss: 0.48338910937309265\n",
      "Epoch: 1499 Loss: 0.48339301347732544\n",
      "Epoch: 1599 Loss: 0.4833727478981018\n",
      "Epoch: 1699 Loss: 0.4833526015281677\n",
      "Epoch: 1799 Loss: 0.4833323359489441\n",
      "Epoch: 1899 Loss: 0.48331207036972046\n",
      "Epoch: 1999 Loss: 0.4832918345928192\n",
      "Epoch: 2099 Loss: 0.48327162861824036\n",
      "Epoch: 2199 Loss: 0.4832513928413391\n",
      "Epoch: 2299 Loss: 0.48323118686676025\n",
      "Epoch: 2399 Loss: 0.4832109808921814\n",
      "Epoch: 2499 Loss: 0.48320674896240234\n",
      "Epoch: 2599 Loss: 0.4832213521003723\n",
      "Epoch: 2699 Loss: 0.4832359552383423\n",
      "Epoch: 2799 Loss: 0.48325061798095703\n",
      "Epoch: 2899 Loss: 0.4832651615142822\n",
      "Epoch: 2999 Loss: 0.483279824256897\n"
     ]
    }
   ],
   "source": [
    "for i in range(3000): # increase number of iterations from 100 to 3000\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(n)\n",
    "    loss = forecast_loss(outputs, sales, stocks)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (i + 1) % 100 == 0: # print in every 100 iterations\n",
    "        #  after 2500 iterations there is a slight increase in loss function, which means gradient starts missing the optimum\n",
    "        print(\"Epoch:\", i, \"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6996e3ab",
   "metadata": {},
   "source": [
    "# Further Findings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604fb775",
   "metadata": {},
   "source": [
    "```diff\n",
    "\n",
    "@@ 1. First of all,  MeanModel is a linear model cannot capture a possible non-linearity between input and output. As such, this model doesn't actually consider any features of the input data and only learns a single, optimal mean value to minimize the custom loss function.\n",
    "\n",
    "@@ 2. Another point to note is that the output of the model should be non-negative as it represents demand, which cannot be negative. However, the MeanModel doesn't guarantee non-negative outputs. Therefore, one can modify the model to ensure that it only produces non-negative outputs, for example by applying an activation function like ReLU.\n",
    "\n",
    "@@ 3. Also, it may be beneficial to consider more complex models or additional features for the input if the performance of this simple MeanModel is not satisfactory.\n",
    "\n",
    "@@ 4. This task only considers the problem of predicting a mean demand value, without any input features, and optimizing this prediction to minimize a custom loss function. In a real-world setting, however, it would be beneficial to consider additional features (such as past sales, time of year, promotions, etc.) and possibly a more complex model to better capture patterns in the data. Furthermore, performance should ideally be evaluated on a separate test set to ensure that the model can generalize well to new, unseen data.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ff89a5",
   "metadata": {},
   "source": [
    "```diff\n",
    "@@ To ensure the model never generates negative values, we can modify the model to pass the output through an  activation function that constrains the output to positive values.\n",
    "\n",
    "@@ One common choice for such an activation function is the ReLU (Rectified Linear Unit) function,  which sets all negative values to zero. However, in a forecasting task such as this, where the output  represents a count (number of items), we might prefer to have our model output be non-zero. In this case,  a better choice of activation function can be the Exponential function, which maps any real number to a positive value.\n",
    "\n",
    "\n",
    "@@ Here is how we can modify the MeanModel to include an exponential activation function:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc8ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonNegativeMeanModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NonNegativeMeanModel, self).__init__()\n",
    "        self.mean = nn.Parameter(torch.randn(1))\n",
    "    \n",
    "    def forward(self, n):\n",
    "        return torch.exp(self.mean) * torch.ones(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f675b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and optimize using gradient descent\n",
    "model = NonNegativeMeanModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cb2ec0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 Loss: 0.5052589178085327\n",
      "Epoch: 199 Loss: 0.4833422303199768\n",
      "Epoch: 299 Loss: 0.483344703912735\n",
      "Epoch: 399 Loss: 0.48334720730781555\n",
      "Epoch: 499 Loss: 0.48334962129592896\n",
      "Epoch: 599 Loss: 0.4833521246910095\n",
      "Epoch: 699 Loss: 0.4833545684814453\n",
      "Epoch: 799 Loss: 0.4833570122718811\n",
      "Epoch: 899 Loss: 0.4833594858646393\n",
      "Epoch: 999 Loss: 0.4833618700504303\n",
      "Epoch: 1099 Loss: 0.4833643436431885\n",
      "Epoch: 1199 Loss: 0.48336678743362427\n",
      "Epoch: 1299 Loss: 0.48336929082870483\n",
      "Epoch: 1399 Loss: 0.483371764421463\n",
      "Epoch: 1499 Loss: 0.4833741784095764\n",
      "Epoch: 1599 Loss: 0.4833766520023346\n",
      "Epoch: 1699 Loss: 0.483379065990448\n",
      "Epoch: 1799 Loss: 0.48338156938552856\n",
      "Epoch: 1899 Loss: 0.48338401317596436\n",
      "Epoch: 1999 Loss: 0.48338645696640015\n",
      "Epoch: 2099 Loss: 0.48338890075683594\n",
      "Epoch: 2199 Loss: 0.4833914041519165\n",
      "Epoch: 2299 Loss: 0.4833937883377075\n",
      "Epoch: 2399 Loss: 0.4833962917327881\n",
      "Epoch: 2499 Loss: 0.4833987355232239\n",
      "Epoch: 2599 Loss: 0.48339658975601196\n",
      "Epoch: 2699 Loss: 0.483393132686615\n",
      "Epoch: 2799 Loss: 0.48338979482650757\n",
      "Epoch: 2899 Loss: 0.48338645696640015\n",
      "Epoch: 2999 Loss: 0.4833829998970032\n"
     ]
    }
   ],
   "source": [
    "for i in range(3000): # increase number of iterations from 100 to 3000\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(n)\n",
    "    loss = forecast_loss(outputs, sales, stocks)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (i + 1) % 100 == 0: # print in every 100 iterations\n",
    "        print(\"Epoch:\", i, \"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d7b25",
   "metadata": {},
   "source": [
    "```diff\n",
    "@@ Now, the NonNegativeMeanModel should always output non-negative values regardless of the learned mean parameter, due to the properties of the exponential function. Please note that using exponential function can make the model harder to optimize and can lead to numerical instability if the values get too large. Hence, while training, we can monitor our loss values and consider decreasing the learning rate if we notice unusually large loss values or oscillations in the loss.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803dcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
